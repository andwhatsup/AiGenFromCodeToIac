{
  "files": [
    {
      "name": "versions.tf",
      "content": "terraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~> 2.25\"\n    }\n  }\n}\n"
    },
    {
      "name": "provider.tf",
      "content": "provider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"terraform\"\n    }\n  }\n}\n\n# Kubernetes provider is configured from the EKS cluster endpoint + auth token.\n# This allows Terraform to apply the Kubernetes Deployment/Service.\nprovider \"kubernetes\" {\n  host                   = module.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n  token                  = data.aws_eks_cluster_auth.this.token\n}\n"
    },
    {
      "name": "variables.tf",
      "content": "variable \"project_name\" {\n  description = \"Project/name prefix used for resources.\"\n  type        = string\n  default     = \"dinosaur\"\n}\n\nvariable \"environment\" {\n  description = \"Environment name (e.g., dev, staging, prod).\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"aws_region\" {\n  description = \"AWS region to deploy into.\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"CIDR block for the VPC.\"\n  type        = string\n  default     = \"10.0.0.0/16\"\n}\n\nvariable \"az_count\" {\n  description = \"Number of AZs to use (2 or 3).\"\n  type        = number\n  default     = 2\n  validation {\n    condition     = var.az_count >= 2 && var.az_count <= 3\n    error_message = \"az_count must be 2 or 3.\"\n  }\n}\n\nvariable \"cluster_version\" {\n  description = \"EKS Kubernetes version.\"\n  type        = string\n  default     = \"1.29\"\n}\n\nvariable \"node_instance_types\" {\n  description = \"Instance types for the EKS managed node group.\"\n  type        = list(string)\n  default     = [\"t3.medium\"]\n}\n\nvariable \"node_desired_size\" {\n  description = \"Desired number of worker nodes.\"\n  type        = number\n  default     = 2\n}\n\nvariable \"node_min_size\" {\n  description = \"Minimum number of worker nodes.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"node_max_size\" {\n  description = \"Maximum number of worker nodes.\"\n  type        = number\n  default     = 4\n}\n\nvariable \"app_image\" {\n  description = \"Container image for the dinosaur app.\"\n  type        = string\n  default     = \"yash5090/dinosaur:latest\"\n}\n\nvariable \"app_replicas\" {\n  description = \"Number of pod replicas for the app.\"\n  type        = number\n  default     = 2\n}\n\nvariable \"app_container_port\" {\n  description = \"Container port the app listens on.\"\n  type        = number\n  default     = 5000\n}\n\nvariable \"service_port\" {\n  description = \"Service port exposed by the Kubernetes Service.\"\n  type        = number\n  default     = 5000\n}\n"
    },
    {
      "name": "main.tf",
      "content": "data \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\nlocals {\n  name = \"${var.project_name}-${var.environment}\"\n\n  azs = slice(data.aws_availability_zones.available.names, 0, var.az_count)\n\n  # Derive /20 subnets from the /16 VPC CIDR.\n  # public:  0..(az_count-1)\n  # private: az_count..(2*az_count-1)\n  public_subnets  = [for i in range(var.az_count) : cidrsubnet(var.vpc_cidr, 4, i)]\n  private_subnets = [for i in range(var.az_count) : cidrsubnet(var.vpc_cidr, 4, i + var.az_count)]\n\n  tags = {\n    Project     = var.project_name\n    Environment = var.environment\n  }\n}\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~> 5.0\"\n\n  name = local.name\n  cidr = var.vpc_cidr\n\n  azs             = local.azs\n  public_subnets  = local.public_subnets\n  private_subnets = local.private_subnets\n\n  enable_nat_gateway     = true\n  single_nat_gateway     = true\n  one_nat_gateway_per_az = false\n\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  # Required for EKS and for AWS Load Balancer Controller / Service type LoadBalancer.\n  public_subnet_tags = {\n    \"kubernetes.io/role/elb\"               = \"1\"\n    \"kubernetes.io/cluster/${local.name}\" = \"shared\"\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/role/internal-elb\"     = \"1\"\n    \"kubernetes.io/cluster/${local.name}\" = \"shared\"\n  }\n\n  tags = local.tags\n}\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 20.0\"\n\n  cluster_name    = local.name\n  cluster_version = var.cluster_version\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  cluster_endpoint_public_access = true\n\n  enable_irsa = true\n\n  # Managed node group for running the app.\n  eks_managed_node_groups = {\n    default = {\n      name           = \"default\"\n      instance_types = var.node_instance_types\n      min_size       = var.node_min_size\n      max_size       = var.node_max_size\n      desired_size   = var.node_desired_size\n\n      # Use private subnets for nodes.\n      subnet_ids = module.vpc.private_subnets\n\n      labels = {\n        role = \"worker\"\n      }\n    }\n  }\n\n  tags = local.tags\n}\n\ndata \"aws_eks_cluster_auth\" \"this\" {\n  name = module.eks.cluster_name\n}\n\n# --- Kubernetes workload (mirrors deployment.yaml/service.yaml) ---\nresource \"kubernetes_namespace\" \"app\" {\n  metadata {\n    name = \"dinosaur\"\n    labels = {\n      app = \"dinosaur\"\n    }\n  }\n}\n\nresource \"kubernetes_deployment\" \"dinosaur\" {\n  metadata {\n    name      = \"dinosaur-deployment\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n    labels = {\n      app = \"dinosaur\"\n    }\n  }\n\n  spec {\n    replicas = var.app_replicas\n\n    selector {\n      match_labels = {\n        app = \"dinosaur\"\n      }\n    }\n\n    template {\n      metadata {\n        labels = {\n          app = \"dinosaur\"\n        }\n      }\n\n      spec {\n        container {\n          name  = \"dinosaur\"\n          image = var.app_image\n\n          port {\n            container_port = var.app_container_port\n          }\n\n          # Basic hardening for a static web container.\n          security_context {\n            allow_privilege_escalation = false\n            read_only_root_filesystem  = false\n            run_as_non_root            = true\n            run_as_user                = 101\n          }\n\n          resources {\n            requests = {\n              cpu    = \"50m\"\n              memory = \"64Mi\"\n            }\n            limits = {\n              cpu    = \"250m\"\n              memory = \"256Mi\"\n            }\n          }\n        }\n      }\n    }\n  }\n\n  depends_on = [module.eks]\n}\n\nresource \"kubernetes_service\" \"dinosaur\" {\n  metadata {\n    name      = \"dinosaur-service\"\n    namespace = kubernetes_namespace.app.metadata[0].name\n\n    # NOTE: In AWS, Service type LoadBalancer typically provisions an NLB.\n    # The repo's service.yaml includes an ALB annotation, but ALB is normally\n    # provisioned via Ingress + AWS Load Balancer Controller.\n    # We keep the annotation to match the repo, but recommend migrating to Ingress.\n    annotations = {\n      \"service.beta.kubernetes.io/aws-load-balancer-backend-protocol\" = \"http\"\n      \"service.beta.kubernetes.io/aws-load-balancer-type\"             = \"alb\"\n    }\n\n    labels = {\n      app = \"dinosaur\"\n    }\n  }\n\n  spec {\n    selector = {\n      app = \"dinosaur\"\n    }\n\n    port {\n      port        = var.service_port\n      target_port = var.app_container_port\n      protocol    = \"TCP\"\n    }\n\n    type = \"LoadBalancer\"\n  }\n\n  depends_on = [kubernetes_deployment.dinosaur]\n}\n"
    },
    {
      "name": "outputs.tf",
      "content": "output \"cluster_name\" {\n  description = \"EKS cluster name.\"\n  value       = module.eks.cluster_name\n}\n\noutput \"cluster_endpoint\" {\n  description = \"EKS cluster API server endpoint.\"\n  value       = module.eks.cluster_endpoint\n}\n\noutput \"vpc_id\" {\n  description = \"VPC ID.\"\n  value       = module.vpc.vpc_id\n}\n\noutput \"service_hostname\" {\n  description = \"External hostname of the Kubernetes Service LoadBalancer (when provisioned).\"\n  value       = try(kubernetes_service.dinosaur.status[0].load_balancer[0].ingress[0].hostname, null)\n}\n"
    },
    {
      "name": "terraform.tfvars",
      "content": "project_name = \"dinosaur\"\nenvironment  = \"dev\"\naws_region   = \"us-east-1\"\n\n# Keep defaults unless you need to customize\n# vpc_cidr = \"10.0.0.0/16\"\n# az_count = 2\n# cluster_version = \"1.29\"\n\napp_image = \"yash5090/dinosaur:latest\"\n"
    },
    {
      "name": "README.md",
      "content": "# Terraform: Dinosaur app on EKS\n\nThis Terraform configuration provisions:\n- A VPC (public + private subnets, NAT)\n- An EKS cluster with a managed node group\n- A Kubernetes namespace, Deployment (replicas=2), and Service type LoadBalancer\n\n## Usage\n\n```bash\ncd ai_tf\nterraform init\nterraform validate\nterraform plan\n```\n\n> Note: The Kubernetes `Service` annotation `aws-load-balancer-type: alb` is kept to match the repo manifests, but on EKS an ALB is typically created via **Ingress** + **AWS Load Balancer Controller**. If you want an ALB, consider migrating to an Ingress-based setup.\n"
    }
  ]
}