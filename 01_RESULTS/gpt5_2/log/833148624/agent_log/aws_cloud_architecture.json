{
  "aws_architecture_description": "### Target outcome\nDeploy the same logical system (Nomad server + Nomad client running Redis + Flask web + batch jobs, optionally with Consul and Tailscale) onto AWS in a secure, minimal, and scalable way.\n\n---\n## High-level AWS architecture\n\n### 1) Networking foundation (VPC)\n- Create a **VPC across 2 Availability Zones** for basic HA of the underlying infrastructure.\n- Use **public subnets** only for internet-facing entry points (ALB, optional bastion) and **private subnets** for compute (EC2 instances running Nomad/Consul).\n- Add **NAT Gateway(s)** so private instances can pull container images (Docker Hub/GHCR) and OS packages without being publicly reachable.\n- Use **VPC endpoints** (at least S3, ECR API/DKR, CloudWatch Logs) to reduce NAT traffic and keep traffic private where possible.\n\n### 2) Compute layer: EC2-based Nomad cluster (closest match to the lab)\nBecause the repo is built around **Nomad jobspecs** and a **Nomad server/client split**, the closest AWS deployment is:\n\n- **Nomad Servers (control plane)**\n  - Run **3 Nomad server instances** (recommended for Raft quorum) on **EC2** in private subnets across 2 AZs.\n  - Optionally run **Consul agents** on the same instances (or as separate instances) if you want Consul-backed service discovery.\n  - Store Nomad/Consul data on **EBS** volumes.\n\n- **Nomad Clients (workload plane)**\n  - Run a small **Auto Scaling Group** of EC2 instances in private subnets.\n  - Install Docker and run **Nomad client** with the Docker driver.\n  - Nomad schedules the workloads:\n    - **Redis** container\n    - **Flask Web API** container (port 5000)\n    - **Employee worker** periodic/batch job\n    - **Setup** parameterized batch job\n\n**Bootstrapping/configuration approach**\n- Replace the local Vagrant/Docker Compose “node creation” with:\n  - **EC2 Launch Templates + user data** to install Nomad/Consul/Docker and write `server.hcl` / `client.hcl`.\n  - Optionally keep **Ansible** as a post-provision step (run from a CI runner or a management host) if you want to preserve the playbook workflow.\n\n### 3) Container image strategy\n- Mirror/publish the application images into **Amazon ECR** (recommended) to avoid rate limits and improve reliability.\n- Nomad clients pull images from ECR using an **instance profile** (IAM role).\n\n### 4) Service exposure (Web API)\n- Put an **Application Load Balancer (ALB)** in public subnets.\n- ALB forwards to the web service running on Nomad clients.\n  - Simplest pattern: run the web job on a **fixed host port** (e.g., 5000) on clients and register the EC2 instances in an **ALB Target Group**.\n  - More dynamic pattern (optional): use **Nomad service discovery + Consul** and a sidecar/proxy (e.g., Fabio/Traefik) to avoid fixed ports. Keep it lean unless you need it.\n\n### 5) Data layer (Redis)\nTwo viable options:\n- **Option A (closest to lab): Redis as a Nomad job** on the client nodes.\n  - Fast to replicate the lab.\n  - You must handle persistence/HA yourself.\n- **Option B (recommended for production): Amazon ElastiCache for Redis**\n  - Offloads patching, backups (where applicable), failover.\n  - Nomad jobs (web/worker/setup) connect to the ElastiCache endpoint.\n\nGiven the repo is a learning/lab setup, Option A is acceptable; for real environments, Option B is typically the better AWS-native choice.\n\n### 6) Secrets, config, and state\n- Store sensitive values (Nomad gossip key, Consul encryption key, app secrets) in **AWS Secrets Manager** or **SSM Parameter Store**.\n- Store logs in **CloudWatch Logs** (via CloudWatch Agent or container logging).\n- Store artifacts (optional) in **S3** (e.g., rendered configs, backups, job bundles).\n\n### 7) Access and operations\n- Use **AWS Systems Manager (SSM) Session Manager** for admin access to instances (avoid SSH/bastion if possible).\n- If you still need SSH, use a **bastion host** in a public subnet with tight security group rules.\n- Monitoring/alerting via **CloudWatch metrics/alarms**; optionally add **AWS Managed Grafana** later.\n\n### 8) Tailscale overlay (optional)\n- If you need the same “overlay VPN” behavior, run **Tailscale** on the EC2 instances (systemd service or container).\n- Control access with security groups first; use Tailscale mainly for operator convenience or cross-network connectivity.\n\n---\n## Relationships / traffic flows\n- Admin/CI → (SSM Session Manager) → EC2 instances (no inbound SSH required)\n- Internet users → ALB (HTTPS 443) → Nomad client instances (web service port)\n- Web/worker/setup → Redis (either on Nomad or ElastiCache) within private subnets\n- Nomad clients ↔ Nomad servers over Nomad RPC (private)\n- Optional: Consul gossip/service discovery traffic within private subnets\n\n---\n## Deployment strategy (lean)\n1. Build/push images to **ECR**.\n2. Provision VPC + endpoints + IAM.\n3. Launch **Nomad server** EC2 instances (3) and **Nomad client** ASG.\n4. Submit Nomad jobspecs (CI pipeline step or a management host).\n5. Expose web via **ALB**.\n\n",
  "aws_resources": [
    "VPC",
    "Internet Gateway (IGW)",
    "Public Subnets (2 AZs)",
    "Private Subnets (2 AZs)",
    "Route Tables (public/private)",
    "NAT Gateway (1 or 2)",
    "Elastic IPs (for NAT Gateway)",
    "VPC Endpoints: S3 (Gateway)",
    "VPC Endpoints: ECR API (Interface)",
    "VPC Endpoints: ECR DKR (Interface)",
    "VPC Endpoints: CloudWatch Logs (Interface)",
    "Security Groups (ALB, Nomad servers, Nomad clients, optional bastion)",
    "Network ACLs (optional; if using beyond SGs)",
    "Application Load Balancer (ALB)",
    "ALB Target Group",
    "ALB Listener (HTTPS 443)",
    "ACM Certificate (for ALB TLS)",
    "Route 53 Hosted Zone + DNS Record (optional, if using a domain)",
    "EC2 Launch Template (Nomad servers)",
    "EC2 Instances (Nomad servers; typically 3)",
    "EBS Volumes (Nomad/Consul data)",
    "EC2 Launch Template (Nomad clients)",
    "EC2 Auto Scaling Group (Nomad clients)",
    "IAM Role/Instance Profile for Nomad servers (SSM, CloudWatch, ECR pull as needed)",
    "IAM Role/Instance Profile for Nomad clients (SSM, CloudWatch, ECR pull)",
    "IAM Policies (least-privilege for ECR, SSM, CloudWatch, Secrets)",
    "AWS Systems Manager (SSM) configuration (Session Manager, managed instance core)",
    "CloudWatch Log Groups (system/app logs)",
    "CloudWatch Agent (on EC2)",
    "CloudWatch Alarms (CPU, disk, instance health, ALB 5xx)",
    "Amazon ECR Repositories (for web/worker/setup images; optionally redis mirror)",
    "Secrets Manager or SSM Parameter Store (Nomad/Consul keys, app secrets)",
    "S3 Bucket (optional: artifacts/config backups)",
    "KMS Key (for encrypting Secrets Manager/SSM parameters and optionally EBS/S3)",
    "ElastiCache for Redis (optional alternative to running Redis on Nomad)",
    "Tailscale deployment mechanism (optional: user data/systemd or container)",
    "Bastion Host EC2 (optional, only if SSH is required)"
  ]
}