{
  "files": [
    {
      "name": "provider.tf",
      "content": "terraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"terraform\"\n    }\n  }\n}\n"
    },
    {
      "name": "variables.tf",
      "content": "variable \"project_name\" {\n  description = \"Project/name prefix used for resource naming.\"\n  type        = string\n  default     = \"nomad-lab\"\n}\n\nvariable \"environment\" {\n  description = \"Environment name (e.g., dev, staging, prod).\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"aws_region\" {\n  description = \"AWS region.\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR.\"\n  type        = string\n  default     = \"10.20.0.0/16\"\n}\n\nvariable \"az_count\" {\n  description = \"Number of AZs to use (2 recommended).\"\n  type        = number\n  default     = 2\n}\n\nvariable \"public_subnet_cidrs\" {\n  description = \"Public subnet CIDRs (length must match az_count).\"\n  type        = list(string)\n  default     = [\"10.20.0.0/24\", \"10.20.1.0/24\"]\n}\n\nvariable \"private_subnet_cidrs\" {\n  description = \"Private subnet CIDRs (length must match az_count).\"\n  type        = list(string)\n  default     = [\"10.20.10.0/24\", \"10.20.11.0/24\"]\n}\n\nvariable \"enable_vpc_endpoints\" {\n  description = \"Create VPC endpoints for S3, ECR (api/dkr), and CloudWatch Logs.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"key_pair_name\" {\n  description = \"Optional EC2 key pair name for break-glass SSH. Leave null to disable SSH key injection.\"\n  type        = string\n  default     = null\n}\n\nvariable \"allowed_ssh_cidrs\" {\n  description = \"If key_pair_name is set, allow SSH from these CIDRs to instances (not recommended; prefer SSM).\"\n  type        = list(string)\n  default     = []\n}\n\nvariable \"nomad_server_instance_type\" {\n  description = \"Instance type for Nomad servers.\"\n  type        = string\n  default     = \"t3.small\"\n}\n\nvariable \"nomad_client_instance_type\" {\n  description = \"Instance type for Nomad clients.\"\n  type        = string\n  default     = \"t3.small\"\n}\n\nvariable \"nomad_server_count\" {\n  description = \"Number of Nomad server instances (3 recommended for Raft quorum).\"\n  type        = number\n  default     = 3\n}\n\nvariable \"nomad_client_desired\" {\n  description = \"Desired number of Nomad client instances.\"\n  type        = number\n  default     = 2\n}\n\nvariable \"nomad_client_min\" {\n  description = \"Min number of Nomad client instances.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"nomad_client_max\" {\n  description = \"Max number of Nomad client instances.\"\n  type        = number\n  default     = 4\n}\n\nvariable \"nomad_version\" {\n  description = \"Nomad version to install.\"\n  type        = string\n  default     = \"1.8.2\"\n}\n\nvariable \"consul_version\" {\n  description = \"Consul version to install (optional).\"\n  type        = string\n  default     = \"1.19.1\"\n}\n\nvariable \"install_consul\" {\n  description = \"Whether to install and run Consul agent alongside Nomad.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enable_alb\" {\n  description = \"Create an internet-facing ALB and target group for the web service.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"web_service_port\" {\n  description = \"Host port on Nomad clients where the web service listens (fixed port simplifies ALB).\"\n  type        = number\n  default     = 5000\n}\n\nvariable \"acm_certificate_arn\" {\n  description = \"ACM certificate ARN for HTTPS listener. If null, an HTTP listener on 80 is created instead.\"\n  type        = string\n  default     = null\n}\n\nvariable \"create_ecr_repositories\" {\n  description = \"Create ECR repositories for app images (web/employee/setup).\"\n  type        = bool\n  default     = true\n}\n"
    },
    {
      "name": "main.tf",
      "content": "data \"aws_caller_identity\" \"current\" {}\n\ndata \"aws_partition\" \"current\" {}\n\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\nlocals {\n  name_prefix = \"${var.project_name}-${var.environment}\"\n  azs         = slice(data.aws_availability_zones.available.names, 0, var.az_count)\n\n  public_subnet_cidrs  = slice(var.public_subnet_cidrs, 0, var.az_count)\n  private_subnet_cidrs = slice(var.private_subnet_cidrs, 0, var.az_count)\n}\n\n# ----------------------------\n# Networking (VPC)\n# ----------------------------\nresource \"aws_vpc\" \"this\" {\n  cidr_block           = var.vpc_cidr\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"${local.name_prefix}-vpc\"\n  }\n}\n\nresource \"aws_internet_gateway\" \"this\" {\n  vpc_id = aws_vpc.this.id\n\n  tags = {\n    Name = \"${local.name_prefix}-igw\"\n  }\n}\n\nresource \"aws_subnet\" \"public\" {\n  for_each = { for idx, az in local.azs : idx => az }\n\n  vpc_id                  = aws_vpc.this.id\n  availability_zone       = each.value\n  cidr_block              = local.public_subnet_cidrs[tonumber(each.key)]\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"${local.name_prefix}-public-${each.value}\"\n    Tier = \"public\"\n  }\n}\n\nresource \"aws_subnet\" \"private\" {\n  for_each = { for idx, az in local.azs : idx => az }\n\n  vpc_id                  = aws_vpc.this.id\n  availability_zone       = each.value\n  cidr_block              = local.private_subnet_cidrs[tonumber(each.key)]\n  map_public_ip_on_launch = false\n\n  tags = {\n    Name = \"${local.name_prefix}-private-${each.value}\"\n    Tier = \"private\"\n  }\n}\n\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.this.id\n\n  tags = {\n    Name = \"${local.name_prefix}-public-rt\"\n  }\n}\n\nresource \"aws_route\" \"public_inet\" {\n  route_table_id         = aws_route_table.public.id\n  destination_cidr_block = \"0.0.0.0/0\"\n  gateway_id             = aws_internet_gateway.this.id\n}\n\nresource \"aws_route_table_association\" \"public\" {\n  for_each = aws_subnet.public\n\n  subnet_id      = each.value.id\n  route_table_id = aws_route_table.public.id\n}\n\nresource \"aws_eip\" \"nat\" {\n  domain = \"vpc\"\n\n  tags = {\n    Name = \"${local.name_prefix}-nat-eip\"\n  }\n}\n\nresource \"aws_nat_gateway\" \"this\" {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = values(aws_subnet.public)[0].id\n\n  tags = {\n    Name = \"${local.name_prefix}-nat\"\n  }\n\n  depends_on = [aws_internet_gateway.this]\n}\n\nresource \"aws_route_table\" \"private\" {\n  vpc_id = aws_vpc.this.id\n\n  tags = {\n    Name = \"${local.name_prefix}-private-rt\"\n  }\n}\n\nresource \"aws_route\" \"private_nat\" {\n  route_table_id         = aws_route_table.private.id\n  destination_cidr_block = \"0.0.0.0/0\"\n  nat_gateway_id         = aws_nat_gateway.this.id\n}\n\nresource \"aws_route_table_association\" \"private\" {\n  for_each = aws_subnet.private\n\n  subnet_id      = each.value.id\n  route_table_id = aws_route_table.private.id\n}\n\n# ----------------------------\n# VPC Endpoints (optional)\n# ----------------------------\nresource \"aws_security_group\" \"vpce\" {\n  name        = \"${local.name_prefix}-vpce\"\n  description = \"Security group for interface VPC endpoints\"\n  vpc_id      = aws_vpc.this.id\n\n  ingress {\n    description = \"HTTPS from VPC\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"${local.name_prefix}-vpce\"\n  }\n}\n\nresource \"aws_vpc_endpoint\" \"s3\" {\n  count = var.enable_vpc_endpoints ? 1 : 0\n\n  vpc_id            = aws_vpc.this.id\n  service_name      = \"com.amazonaws.${var.aws_region}.s3\"\n  vpc_endpoint_type = \"Gateway\"\n  route_table_ids   = [aws_route_table.private.id]\n\n  tags = {\n    Name = \"${local.name_prefix}-s3-endpoint\"\n  }\n}\n\nresource \"aws_vpc_endpoint\" \"ecr_api\" {\n  count = var.enable_vpc_endpoints ? 1 : 0\n\n  vpc_id              = aws_vpc.this.id\n  service_name        = \"com.amazonaws.${var.aws_region}.ecr.api\"\n  vpc_endpoint_type   = \"Interface\"\n  private_dns_enabled = true\n  subnet_ids          = [for s in aws_subnet.private : s.id]\n  security_group_ids  = [aws_security_group.vpce.id]\n\n  tags = {\n    Name = \"${local.name_prefix}-ecr-api-endpoint\"\n  }\n}\n\nresource \"aws_vpc_endpoint\" \"ecr_dkr\" {\n  count = var.enable_vpc_endpoints ? 1 : 0\n\n  vpc_id              = aws_vpc.this.id\n  service_name        = \"com.amazonaws.${var.aws_region}.ecr.dkr\"\n  vpc_endpoint_type   = \"Interface\"\n  private_dns_enabled = true\n  subnet_ids          = [for s in aws_subnet.private : s.id]\n  security_group_ids  = [aws_security_group.vpce.id]\n\n  tags = {\n    Name = \"${local.name_prefix}-ecr-dkr-endpoint\"\n  }\n}\n\nresource \"aws_vpc_endpoint\" \"logs\" {\n  count = var.enable_vpc_endpoints ? 1 : 0\n\n  vpc_id              = aws_vpc.this.id\n  service_name        = \"com.amazonaws.${var.aws_region}.logs\"\n  vpc_endpoint_type   = \"Interface\"\n  private_dns_enabled = true\n  subnet_ids          = [for s in aws_subnet.private : s.id]\n  security_group_ids  = [aws_security_group.vpce.id]\n\n  tags = {\n    Name = \"${local.name_prefix}-logs-endpoint\"\n  }\n}\n"
    },
    {
      "name": "iam.tf",
      "content": "# IAM for EC2 instances (SSM + CloudWatch + ECR pull)\n\ndata \"aws_iam_policy_document\" \"ec2_assume_role\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"sts:AssumeRole\",\n    ]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"ec2.amazonaws.com\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"nomad\" {\n  name               = \"${var.project_name}-${var.environment}-nomad-ec2\"\n  assume_role_policy = data.aws_iam_policy_document.ec2_assume_role.json\n}\n\nresource \"aws_iam_instance_profile\" \"nomad\" {\n  name = \"${var.project_name}-${var.environment}-nomad-ec2\"\n  role = aws_iam_role.nomad.name\n}\n\n# SSM managed instance core\nresource \"aws_iam_role_policy_attachment\" \"ssm\" {\n  role       = aws_iam_role.nomad.name\n  policy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\n\n# CloudWatch agent\nresource \"aws_iam_role_policy_attachment\" \"cw_agent\" {\n  role       = aws_iam_role.nomad.name\n  policy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/CloudWatchAgentServerPolicy\"\n}\n\n# ECR read-only (pull)\nresource \"aws_iam_role_policy_attachment\" \"ecr_read\" {\n  role       = aws_iam_role.nomad.name\n  policy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n}\n"
    },
    {
      "name": "security_groups.tf",
      "content": "locals {\n  # Nomad ports (default)\n  nomad_rpc_port  = 4647\n  nomad_http_port = 4646\n  nomad_serf_port = 4648\n\n  consul_http_port = 8500\n  consul_serf_lan  = 8301\n  consul_server    = 8300\n}\n\nresource \"aws_security_group\" \"alb\" {\n  count = var.enable_alb ? 1 : 0\n\n  name        = \"${local.name_prefix}-alb\"\n  description = \"ALB security group\"\n  vpc_id      = aws_vpc.this.id\n\n  ingress {\n    description = \"HTTP\"\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  dynamic \"ingress\" {\n    for_each = var.acm_certificate_arn != null ? [1] : []\n    content {\n      description = \"HTTPS\"\n      from_port   = 443\n      to_port     = 443\n      protocol    = \"tcp\"\n      cidr_blocks = [\"0.0.0.0/0\"]\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"${local.name_prefix}-alb\"\n  }\n}\n\nresource \"aws_security_group\" \"nomad_servers\" {\n  name        = \"${local.name_prefix}-nomad-servers\"\n  description = \"Nomad server security group\"\n  vpc_id      = aws_vpc.this.id\n\n  # Nomad intra-cluster\n  ingress {\n    description = \"Nomad RPC\"\n    from_port   = local.nomad_rpc_port\n    to_port     = local.nomad_rpc_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  ingress {\n    description = \"Nomad HTTP\"\n    from_port   = local.nomad_http_port\n    to_port     = local.nomad_http_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  ingress {\n    description = \"Nomad Serf\"\n    from_port   = local.nomad_serf_port\n    to_port     = local.nomad_serf_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  # Optional Consul\n  dynamic \"ingress\" {\n    for_each = var.install_consul ? [1] : []\n    content {\n      description = \"Consul HTTP\"\n      from_port   = local.consul_http_port\n      to_port     = local.consul_http_port\n      protocol    = \"tcp\"\n      cidr_blocks = [aws_vpc.this.cidr_block]\n    }\n  }\n\n  dynamic \"ingress\" {\n    for_each = var.install_consul ? [1] : []\n    content {\n      description = \"Consul server RPC\"\n      from_port   = local.consul_server\n      to_port     = local.consul_server\n      protocol    = \"tcp\"\n      cidr_blocks = [aws_vpc.this.cidr_block]\n    }\n  }\n\n  dynamic \"ingress\" {\n    for_each = var.install_consul ? [1] : []\n    content {\n      description = \"Consul Serf LAN\"\n      from_port   = local.consul_serf_lan\n      to_port     = local.consul_serf_lan\n      protocol    = \"tcp\"\n      cidr_blocks = [aws_vpc.this.cidr_block]\n    }\n  }\n\n  # SSH (optional)\n  dynamic \"ingress\" {\n    for_each = var.key_pair_name != null && length(var.allowed_ssh_cidrs) > 0 ? [1] : []\n    content {\n      description = \"SSH (break-glass)\"\n      from_port   = 22\n      to_port     = 22\n      protocol    = \"tcp\"\n      cidr_blocks = var.allowed_ssh_cidrs\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"${local.name_prefix}-nomad-servers\"\n  }\n}\n\nresource \"aws_security_group\" \"nomad_clients\" {\n  name        = \"${local.name_prefix}-nomad-clients\"\n  description = \"Nomad client security group\"\n  vpc_id      = aws_vpc.this.id\n\n  # Allow Nomad client to talk to servers and other clients\n  ingress {\n    description = \"Nomad RPC\"\n    from_port   = local.nomad_rpc_port\n    to_port     = local.nomad_rpc_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  ingress {\n    description = \"Nomad HTTP\"\n    from_port   = local.nomad_http_port\n    to_port     = local.nomad_http_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  ingress {\n    description = \"Nomad Serf\"\n    from_port   = local.nomad_serf_port\n    to_port     = local.nomad_serf_port\n    protocol    = \"tcp\"\n    cidr_blocks = [aws_vpc.this.cidr_block]\n  }\n\n  # Web service port from ALB\n  dynamic \"ingress\" {\n    for_each = var.enable_alb ? [1] : []\n    content {\n      description     = \"Web from ALB\"\n      from_port       = var.web_service_port\n      to_port         = var.web_service_port\n      protocol        = \"tcp\"\n      security_groups = [aws_security_group.alb[0].id]\n    }\n  }\n\n  # SSH (optional)\n  dynamic \"ingress\" {\n    for_each = var.key_pair_name != null && length(var.allowed_ssh_cidrs) > 0 ? [1] : []\n    content {\n      description = \"SSH (break-glass)\"\n      from_port   = 22\n      to_port     = 22\n      protocol    = \"tcp\"\n      cidr_blocks = var.allowed_ssh_cidrs\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"${local.name_prefix}-nomad-clients\"\n  }\n}\n"
    },
    {
      "name": "compute.tf",
      "content": "data \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nresource \"aws_cloudwatch_log_group\" \"system\" {\n  name              = \"/${local.name_prefix}/system\"\n  retention_in_days = 14\n}\n\nlocals {\n  # Cloud-init/user-data scripts are kept inline for simplicity.\n  # They install Docker + Nomad (and optionally Consul) and configure systemd services.\n\n  server_user_data = <<-EOF\n    #!/bin/bash\n    set -euo pipefail\n\n    export DEBIAN_FRONTEND=noninteractive\n\n    apt-get update -y\n    apt-get install -y unzip curl jq ca-certificates gnupg lsb-release\n\n    # Install Docker\n    install -m 0755 -d /etc/apt/keyrings\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n    chmod a+r /etc/apt/keyrings/docker.gpg\n    echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker.list\n    apt-get update -y\n    apt-get install -y docker-ce docker-ce-cli containerd.io\n\n    systemctl enable docker\n    systemctl start docker\n\n    # Install Nomad\n    NOMAD_VERSION=\"${var.nomad_version}\"\n    curl -fsSLo /tmp/nomad.zip \"https://releases.hashicorp.com/nomad/${var.nomad_version}/nomad_${var.nomad_version}_linux_amd64.zip\"\n    unzip -o /tmp/nomad.zip -d /usr/local/bin\n    chmod +x /usr/local/bin/nomad\n\n    useradd --system --home /etc/nomad.d --shell /bin/false nomad || true\n    mkdir -p /etc/nomad.d /opt/nomad\n    chown -R nomad:nomad /opt/nomad\n\n    cat >/etc/nomad.d/nomad.hcl <<'HCL'\n    data_dir  = \"/opt/nomad\"\n    bind_addr = \"0.0.0.0\"\n\n    server {\n      enabled          = true\n      bootstrap_expect = ${var.nomad_server_count}\n    }\n\n    advertise {\n      http = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n      rpc  = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n      serf = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n    }\n\n    client {\n      enabled = false\n    }\n\n    ui {\n      enabled = true\n    }\n\n    # Join via AWS cloud auto-join (requires EC2 DescribeInstances permission; we keep it simple and use static retry_join via private DNS)\n    retry_join = [\"provider=aws tag_key=NomadCluster tag_value=${local.name_prefix}\"]\n    HCL\n\n    cat >/etc/systemd/system/nomad.service <<'UNIT'\n    [Unit]\n    Description=Nomad\n    Documentation=https://www.nomadproject.io/docs/\n    Wants=network-online.target\n    After=network-online.target\n\n    [Service]\n    User=nomad\n    Group=nomad\n    ExecStart=/usr/local/bin/nomad agent -config=/etc/nomad.d\n    ExecReload=/bin/kill -HUP $MAINPID\n    KillMode=process\n    KillSignal=SIGINT\n    LimitNOFILE=65536\n    Restart=on-failure\n\n    [Install]\n    WantedBy=multi-user.target\n    UNIT\n\n    systemctl daemon-reload\n    systemctl enable nomad\n    systemctl start nomad\n\n    EOF\n\n  client_user_data = <<-EOF\n    #!/bin/bash\n    set -euo pipefail\n\n    export DEBIAN_FRONTEND=noninteractive\n\n    apt-get update -y\n    apt-get install -y unzip curl jq ca-certificates gnupg lsb-release\n\n    # Install Docker\n    install -m 0755 -d /etc/apt/keyrings\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n    chmod a+r /etc/apt/keyrings/docker.gpg\n    echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker.list\n    apt-get update -y\n    apt-get install -y docker-ce docker-ce-cli containerd.io\n\n    systemctl enable docker\n    systemctl start docker\n\n    # Install Nomad\n    curl -fsSLo /tmp/nomad.zip \"https://releases.hashicorp.com/nomad/${var.nomad_version}/nomad_${var.nomad_version}_linux_amd64.zip\"\n    unzip -o /tmp/nomad.zip -d /usr/local/bin\n    chmod +x /usr/local/bin/nomad\n\n    useradd --system --home /etc/nomad.d --shell /bin/false nomad || true\n    mkdir -p /etc/nomad.d /opt/nomad\n    chown -R nomad:nomad /opt/nomad\n\n    cat >/etc/nomad.d/nomad.hcl <<'HCL'\n    data_dir  = \"/opt/nomad\"\n    bind_addr = \"0.0.0.0\"\n\n    client {\n      enabled = true\n    }\n\n    plugin \"docker\" {\n      config {\n        allow_privileged = true\n      }\n    }\n\n    advertise {\n      http = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n      rpc  = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n      serf = \"{{ GetInterfaceIP \\\"eth0\\\" }}\"\n    }\n\n    retry_join = [\"provider=aws tag_key=NomadCluster tag_value=${local.name_prefix}\"]\n    HCL\n\n    cat >/etc/systemd/system/nomad.service <<'UNIT'\n    [Unit]\n    Description=Nomad\n    Documentation=https://www.nomadproject.io/docs/\n    Wants=network-online.target\n    After=network-online.target\n\n    [Service]\n    User=nomad\n    Group=nomad\n    ExecStart=/usr/local/bin/nomad agent -config=/etc/nomad.d\n    ExecReload=/bin/kill -HUP $MAINPID\n    KillMode=process\n    KillSignal=SIGINT\n    LimitNOFILE=65536\n    Restart=on-failure\n\n    [Install]\n    WantedBy=multi-user.target\n    UNIT\n\n    systemctl daemon-reload\n    systemctl enable nomad\n    systemctl start nomad\n\n    EOF\n}\n\n# Allow Nomad auto-join to discover instances by tag\nresource \"aws_iam_role_policy\" \"nomad_autojoin\" {\n  name = \"${local.name_prefix}-nomad-autojoin\"\n  role = aws_iam_role.nomad.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"ec2:DescribeInstances\",\n          \"ec2:DescribeTags\",\n          \"ec2:DescribeRegions\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n\nresource \"aws_launch_template\" \"nomad_server\" {\n  name_prefix   = \"${local.name_prefix}-server-\"\n  image_id      = data.aws_ami.ubuntu.id\n  instance_type = var.nomad_server_instance_type\n\n  key_name = var.key_pair_name\n\n  iam_instance_profile {\n    name = aws_iam_instance_profile.nomad.name\n  }\n\n  vpc_security_group_ids = [aws_security_group.nomad_servers.id]\n\n  user_data = base64encode(local.server_user_data)\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  block_device_mappings {\n    device_name = \"/dev/sda1\"\n\n    ebs {\n      volume_size           = 30\n      volume_type           = \"gp3\"\n      encrypted             = true\n      delete_on_termination = true\n    }\n  }\n\n  tag_specifications {\n    resource_type = \"instance\"\n    tags = {\n      Name        = \"${local.name_prefix}-nomad-server\"\n      Role        = \"nomad-server\"\n      NomadCluster = local.name_prefix\n    }\n  }\n\n  tag_specifications {\n    resource_type = \"volume\"\n    tags = {\n      Name = \"${local.name_prefix}-nomad-server\"\n    }\n  }\n}\n\nresource \"aws_autoscaling_group\" \"nomad_servers\" {\n  name                = \"${local.name_prefix}-nomad-servers\"\n  desired_capacity    = var.nomad_server_count\n  max_size            = var.nomad_server_count\n  min_size            = var.nomad_server_count\n  vpc_zone_identifier = [for s in aws_subnet.private : s.id]\n\n  health_check_type         = \"EC2\"\n  health_check_grace_period = 120\n\n  launch_template {\n    id      = aws_launch_template.nomad_server.id\n    version = \"$Latest\"\n  }\n\n  termination_policies = [\"OldestInstance\"]\n\n  tag {\n    key                 = \"NomadCluster\"\n    value               = local.name_prefix\n    propagate_at_launch = true\n  }\n\n  tag {\n    key                 = \"Role\"\n    value               = \"nomad-server\"\n    propagate_at_launch = true\n  }\n}\n\nresource \"aws_launch_template\" \"nomad_client\" {\n  name_prefix   = \"${local.name_prefix}-client-\"\n  image_id      = data.aws_ami.ubuntu.id\n  instance_type = var.nomad_client_instance_type\n\n  key_name = var.key_pair_name\n\n  iam_instance_profile {\n    name = aws_iam_instance_profile.nomad.name\n  }\n\n  vpc_security_group_ids = [aws_security_group.nomad_clients.id]\n\n  user_data = base64encode(local.client_user_data)\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  block_device_mappings {\n    device_name = \"/dev/sda1\"\n\n    ebs {\n      volume_size           = 30\n      volume_type           = \"gp3\"\n      encrypted             = true\n      delete_on_termination = true\n    }\n  }\n\n  tag_specifications {\n    resource_type = \"instance\"\n    tags = {\n      Name        = \"${local.name_prefix}-nomad-client\"\n      Role        = \"nomad-client\"\n      NomadCluster = local.name_prefix\n    }\n  }\n}\n\nresource \"aws_autoscaling_group\" \"nomad_clients\" {\n  name                = \"${local.name_prefix}-nomad-clients\"\n  desired_capacity    = var.nomad_client_desired\n  max_size            = var.nomad_client_max\n  min_size            = var.nomad_client_min\n  vpc_zone_identifier = [for s in aws_subnet.private : s.id]\n\n  health_check_type         = \"EC2\"\n  health_check_grace_period = 120\n\n  launch_template {\n    id      = aws_launch_template.nomad_client.id\n    version = \"$Latest\"\n  }\n\n  termination_policies = [\"OldestInstance\"]\n\n  tag {\n    key                 = \"NomadCluster\"\n    value               = local.name_prefix\n    propagate_at_launch = true\n  }\n\n  tag {\n    key                 = \"Role\"\n    value               = \"nomad-client\"\n    propagate_at_launch = true\n  }\n}\n"
    },
    {
      "name": "alb.tf",
      "content": "resource \"aws_lb\" \"this\" {\n  count = var.enable_alb ? 1 : 0\n\n  name               = substr(\"${local.name_prefix}-alb\", 0, 32)\n  load_balancer_type = \"application\"\n  internal           = false\n  security_groups    = [aws_security_group.alb[0].id]\n  subnets            = [for s in aws_subnet.public : s.id]\n\n  drop_invalid_header_fields = true\n\n  tags = {\n    Name = \"${local.name_prefix}-alb\"\n  }\n}\n\nresource \"aws_lb_target_group\" \"web\" {\n  count = var.enable_alb ? 1 : 0\n\n  name        = substr(\"${local.name_prefix}-web\", 0, 32)\n  port        = var.web_service_port\n  protocol    = \"HTTP\"\n  vpc_id      = aws_vpc.this.id\n  target_type = \"instance\"\n\n  health_check {\n    enabled             = true\n    interval            = 30\n    path                = \"/\"\n    healthy_threshold   = 2\n    unhealthy_threshold = 3\n    timeout             = 5\n    matcher             = \"200-399\"\n  }\n\n  tags = {\n    Name = \"${local.name_prefix}-web\"\n  }\n}\n\nresource \"aws_autoscaling_attachment\" \"clients_to_tg\" {\n  count = var.enable_alb ? 1 : 0\n\n  autoscaling_group_name = aws_autoscaling_group.nomad_clients.name\n  lb_target_group_arn    = aws_lb_target_group.web[0].arn\n}\n\nresource \"aws_lb_listener\" \"http\" {\n  count = var.enable_alb ? 1 : 0\n\n  load_balancer_arn = aws_lb.this[0].arn\n  port              = 80\n  protocol          = \"HTTP\"\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.web[0].arn\n  }\n}\n\nresource \"aws_lb_listener\" \"https\" {\n  count = var.enable_alb && var.acm_certificate_arn != null ? 1 : 0\n\n  load_balancer_arn = aws_lb.this[0].arn\n  port              = 443\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS13-1-2-2021-06\"\n  certificate_arn   = var.acm_certificate_arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.web[0].arn\n  }\n}\n"
    },
    {
      "name": "ecr.tf",
      "content": "resource \"aws_ecr_repository\" \"web\" {\n  count = var.create_ecr_repositories ? 1 : 0\n\n  name                 = \"${var.project_name}/${var.environment}/pytechco-web\"\n  image_tag_mutability = \"MUTABLE\"\n\n  image_scanning_configuration {\n    scan_on_push = true\n  }\n}\n\nresource \"aws_ecr_repository\" \"employee\" {\n  count = var.create_ecr_repositories ? 1 : 0\n\n  name                 = \"${var.project_name}/${var.environment}/pytechco-employee\"\n  image_tag_mutability = \"MUTABLE\"\n\n  image_scanning_configuration {\n    scan_on_push = true\n  }\n}\n\nresource \"aws_ecr_repository\" \"setup\" {\n  count = var.create_ecr_repositories ? 1 : 0\n\n  name                 = \"${var.project_name}/${var.environment}/pytechco-setup\"\n  image_tag_mutability = \"MUTABLE\"\n\n  image_scanning_configuration {\n    scan_on_push = true\n  }\n}\n"
    },
    {
      "name": "outputs.tf",
      "content": "output \"vpc_id\" {\n  value = aws_vpc.this.id\n}\n\noutput \"public_subnet_ids\" {\n  value = [for s in aws_subnet.public : s.id]\n}\n\noutput \"private_subnet_ids\" {\n  value = [for s in aws_subnet.private : s.id]\n}\n\noutput \"alb_dns_name\" {\n  value       = var.enable_alb ? aws_lb.this[0].dns_name : null\n  description = \"DNS name of the ALB (if enabled).\"\n}\n\noutput \"nomad_servers_asg_name\" {\n  value = aws_autoscaling_group.nomad_servers.name\n}\n\noutput \"nomad_clients_asg_name\" {\n  value = aws_autoscaling_group.nomad_clients.name\n}\n\noutput \"ecr_repository_urls\" {\n  value = var.create_ecr_repositories ? {\n    web      = aws_ecr_repository.web[0].repository_url\n    employee = aws_ecr_repository.employee[0].repository_url\n    setup    = aws_ecr_repository.setup[0].repository_url\n  } : {}\n}\n"
    },
    {
      "name": "terraform.tfvars",
      "content": "project_name = \"nomad-lab\"\nenvironment  = \"dev\"\naws_region   = \"us-east-1\"\n\n# Optional: set to an existing EC2 key pair name for break-glass SSH\n# key_pair_name = \"my-keypair\"\n# allowed_ssh_cidrs = [\"203.0.113.10/32\"]\n\n# Optional: provide ACM cert ARN to enable HTTPS listener on the ALB\n# acm_certificate_arn = \"arn:aws:acm:us-east-1:123456789012:certificate/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n"
    }
  ]
}