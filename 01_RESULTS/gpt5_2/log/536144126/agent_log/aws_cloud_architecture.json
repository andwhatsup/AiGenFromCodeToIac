{
  "aws_architecture_description": "## Target AWS architecture (AWS-managed equivalent of the current Aiven-managed platform)\n\n### 1) Core idea\nReplace the Aiven-managed data platform with AWS-native managed services:\n- **Aiven Kafka → Amazon MSK (Managed Streaming for Apache Kafka)**\n- **Aiven ClickHouse → Amazon Redshift (serverless or provisioned)** for analytics/warehouse-style workloads\n- **Aiven PostgreSQL → Amazon RDS for PostgreSQL** (or Aurora PostgreSQL if you want higher availability options)\n- Keep the “notebook-driven” workflow, but run it in AWS as a managed environment (recommended) rather than on a workstation.\n\nThis yields a secure, scalable, AWS-operated deployment while preserving the same data flow:\nExternal API → producer → Kafka → analytics store → notebook queries/visualization, plus relational reference data in Postgres.\n\n### 2) Networking and security boundary\n**VPC-first design**:\n- Create a dedicated **VPC** with **2–3 Availability Zones**.\n- **Private subnets** host MSK brokers and RDS (no public IPs).\n- **Public subnets** only for load balancers / NAT gateways (if needed).\n- Use **VPC endpoints** (Interface/Gateway) so workloads can reach AWS services (S3, Secrets Manager, CloudWatch, ECR, STS) without traversing the public internet.\n\n**Access patterns**:\n- Analysts access notebooks via **Amazon SageMaker Studio** (or SageMaker Notebook Instances) over HTTPS.\n- If you must support local workstation access, use **AWS Client VPN** (or Site-to-Site VPN/Direct Connect) into the VPC; do not expose MSK/RDS publicly.\n\n### 3) Compute / “application” layer (the notebook + scripts)\n**Recommended**: run notebooks in **Amazon SageMaker Studio**.\n- Store notebooks and artifacts in **S3**.\n- Use **IAM execution roles** for least-privilege access.\n- Store connection strings, MSK bootstrap brokers, DB creds, API keys in **AWS Secrets Manager**.\n\n**Operational scripts (Aiven CLI equivalents)**:\n- Replace Aiven CLI provisioning with **Infrastructure as Code** (AWS CDK / CloudFormation / Terraform).\n- For one-off admin tasks (creating Kafka topics, running SQL migrations), use:\n  - a small **AWS CodeBuild** job, or\n  - a container task in **ECS Fargate**, or\n  - a controlled **EC2 “admin/bastion” instance** in a private subnet (only if necessary).\n\n### 4) Streaming layer (Kafka)\nUse **Amazon MSK**:\n- MSK cluster in private subnets across AZs.\n- Enable **TLS in-transit** and **encryption at rest**.\n- Authentication options:\n  - **IAM authentication** (preferred for AWS-native identity), or\n  - **SASL/SCRAM** with credentials stored in Secrets Manager.\n\n**Producers**:\n- The notebook (SageMaker) produces events to MSK.\n\n### 5) Analytics ingestion and transformation (ClickHouse materialized view equivalent)\nBecause AWS does not provide a first-party managed ClickHouse service, the lean AWS-native equivalent is:\n\n**Option A (recommended, simplest managed analytics): MSK → Redshift via Firehose**\n- Use **Amazon Kinesis Data Firehose** with an **MSK source** to deliver streaming events to:\n  - **Amazon S3** (raw/bronze), and optionally\n  - **Amazon Redshift** (curated/silver) using COPY/auto-ingest patterns.\n- Transformations:\n  - Light transforms in Firehose (Lambda transform) if needed.\n  - Heavier transforms/aggregations in **Redshift SQL** (materialized views in Redshift can approximate the ClickHouse MV pattern).\n\n**Option B (more “stream processing”): MSK → Kinesis Data Analytics (Flink) → S3/Redshift**\n- If you need true streaming aggregations/windowing, use **Kinesis Data Analytics for Apache Flink**.\n\nEither way, keep **S3 as the durable landing zone** for replay/backfill and cost-effective storage.\n\n### 6) Relational/reference data\nUse **Amazon RDS for PostgreSQL**:\n- Multi-AZ deployment for availability.\n- Private subnets only.\n- Credentials in Secrets Manager.\n- Run `scripts/create_postgres_schema.sql` as a migration step (CodeBuild/ECS task/SageMaker bootstrap).\n\n### 7) External API access (Digitransit)\nSageMaker (or ECS tasks) will call the Digitransit Realtime API over HTTPS.\n- If your subnets are private, provide outbound internet via **NAT Gateway**.\n- Add basic controls:\n  - egress security group rules (restrict to 443),\n  - optional caching layer (e.g., ElastiCache) only if you observe rate limits/latency issues.\n\n### 8) Observability, audit, and operations\n- **CloudWatch Logs/Metrics/Alarms** for SageMaker, MSK, Firehose, Lambda, RDS.\n- **AWS CloudTrail** for API auditing.\n- **AWS Config** (optional but recommended in regulated environments).\n- **KMS** keys for encrypting S3, Secrets Manager, RDS, MSK.\n\n### 9) Deployment strategy (lean)\n- Use IaC to provision VPC, MSK, RDS, Redshift, S3, IAM, endpoints.\n- Use a CI/CD pipeline (optional) to:\n  - build/publish a notebook container image (if you containerize),\n  - run SQL migrations,\n  - create Kafka topics.\n\n---\n\n## Notes / tradeoffs\n- If you require ClickHouse specifically (same engine/features), the AWS approach is typically **self-managed ClickHouse on EC2/ECS/EKS**. That is feasible but increases ops burden. The above design stays “managed-first” and lean.\n- If you want a closer “materialized view from Kafka” experience with minimal ops, Redshift materialized views + streaming ingestion (via S3/Firehose) is usually the simplest AWS-native analogue.\n",
  "aws_resources": [
    "Networking: VPC",
    "Networking: Public subnets (2–3 AZs)",
    "Networking: Private subnets (2–3 AZs)",
    "Networking: Route tables (public/private)",
    "Networking: Internet Gateway (IGW)",
    "Networking: NAT Gateway(s) (for private subnet egress to external APIs)",
    "Networking: Security Groups (SageMaker, MSK, RDS, Redshift, admin tasks)",
    "Networking: Network ACLs (optional)",
    "Networking: VPC Endpoints - S3 Gateway Endpoint",
    "Networking: VPC Endpoints - Interface Endpoints (Secrets Manager, CloudWatch Logs, ECR, STS as needed)",
    "Identity: IAM roles (SageMaker execution role, Firehose role, Lambda role, admin/CI roles)",
    "Identity: IAM policies (least privilege)",
    "Security: AWS KMS keys (S3, Secrets Manager, RDS, MSK, Redshift)",
    "Security: AWS Secrets Manager secrets (Postgres creds, MSK auth, Redshift creds, API keys/config)",
    "Analytics storage: Amazon S3 bucket(s) (raw/bronze, curated/silver, notebooks/artifacts)",
    "Analytics storage: S3 bucket policies + default encryption + lifecycle rules",
    "Compute/Notebooks: Amazon SageMaker Studio Domain",
    "Compute/Notebooks: SageMaker User Profiles",
    "Streaming: Amazon MSK cluster",
    "Streaming: MSK configuration (TLS, auth, broker config)",
    "Streaming: MSK topic management mechanism (admin job via CodeBuild/ECS or scripted from SageMaker)",
    "Ingestion: Amazon Kinesis Data Firehose delivery stream (source: MSK, destination: S3 and/or Redshift)",
    "Ingestion: AWS Lambda function (optional) for Firehose record transformation",
    "Warehouse: Amazon Redshift (Serverless workgroup/namespace or provisioned cluster)",
    "Warehouse: Redshift subnet group",
    "Warehouse: Redshift parameter group (optional)",
    "Relational DB: Amazon RDS for PostgreSQL instance/cluster (Multi-AZ)",
    "Relational DB: RDS DB subnet group",
    "Relational DB: RDS parameter group (optional)",
    "Relational DB: RDS automated backups + maintenance window configuration",
    "Ops: Amazon CloudWatch log groups (SageMaker, Firehose, Lambda)",
    "Ops: CloudWatch alarms (RDS CPU/storage, MSK broker health, Firehose delivery failures, Lambda errors)",
    "Ops/Audit: AWS CloudTrail trail",
    "Ops: AWS Backup plan (optional, for RDS and other supported resources)",
    "Optional secure access: AWS Client VPN endpoint (if analysts connect from local workstations)",
    "Optional admin: EC2 bastion/admin instance in private subnet (only if needed)",
    "Optional CI/CD: AWS CodeBuild project (run migrations, create topics, deploy artifacts)"
  ]
}