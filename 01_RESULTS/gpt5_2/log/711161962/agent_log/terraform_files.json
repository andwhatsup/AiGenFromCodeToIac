{
  "files": [
    {
      "name": "versions.tf",
      "content": "terraform {\n  required_version = \">= 1.5.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n"
    },
    {
      "name": "provider.tf",
      "content": "provider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = var.tags\n  }\n}\n"
    },
    {
      "name": "variables.tf",
      "content": "variable \"aws_region\" {\n  description = \"AWS region to deploy into.\"\n  type        = string\n  default     = \"ap-southeast-1\"\n}\n\nvariable \"name_prefix\" {\n  description = \"Prefix used for naming AWS resources.\"\n  type        = string\n  default     = \"localstack-s3-sqs-lambda\"\n}\n\nvariable \"lambda_artifact_path\" {\n  description = \"Path to the Lambda deployment package (zip) containing the bootstrap binary.\"\n  type        = string\n  default     = \"../lambda/build/handler.zip\"\n}\n\nvariable \"lambda_memory_size\" {\n  description = \"Lambda memory size (MB).\"\n  type        = number\n  default     = 128\n}\n\nvariable \"lambda_timeout_seconds\" {\n  description = \"Lambda timeout in seconds.\"\n  type        = number\n  default     = 30\n}\n\nvariable \"log_retention_days\" {\n  description = \"CloudWatch log group retention in days.\"\n  type        = number\n  default     = 14\n}\n\nvariable \"s3_event_prefix\" {\n  description = \"Optional S3 key prefix filter for notifications (e.g., 'incoming/'). Empty means no prefix filter.\"\n  type        = string\n  default     = \"\"\n}\n\nvariable \"s3_event_suffix\" {\n  description = \"Optional S3 key suffix filter for notifications (e.g., '.csv'). Empty means no suffix filter.\"\n  type        = string\n  default     = \".csv\"\n}\n\nvariable \"sqs_visibility_timeout_seconds\" {\n  description = \"Visibility timeout for the main SQS queue. Should be >= Lambda timeout.\"\n  type        = number\n  default     = 60\n}\n\nvariable \"sqs_message_retention_seconds\" {\n  description = \"How long SQS retains messages.\"\n  type        = number\n  default     = 345600 # 4 days\n}\n\nvariable \"sqs_max_receive_count\" {\n  description = \"How many times a message can be received before moving to DLQ.\"\n  type        = number\n  default     = 5\n}\n\nvariable \"lambda_sqs_batch_size\" {\n  description = \"Number of messages to send to the Lambda function in each batch.\"\n  type        = number\n  default     = 10\n}\n\nvariable \"lambda_sqs_maximum_batching_window_seconds\" {\n  description = \"Maximum batching window in seconds for SQS event source mapping.\"\n  type        = number\n  default     = 0\n}\n\nvariable \"enable_s3_read\" {\n  description = \"Whether to grant Lambda permission to read objects from the S3 bucket.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"tags\" {\n  description = \"Tags to apply to all resources.\"\n  type        = map(string)\n  default = {\n    Project = \"localstack-s3-sqs-lambda\"\n    Managed = \"terraform\"\n  }\n}\n"
    },
    {
      "name": "main.tf",
      "content": "locals {\n  bucket_name = lower(replace(\"${var.name_prefix}-${data.aws_caller_identity.current.account_id}-${var.aws_region}\", \"_\", \"-\"))\n}\n\ndata \"aws_caller_identity\" \"current\" {}\n\ndata \"aws_partition\" \"current\" {}\n\ndata \"aws_region\" \"current\" {}\n\n# -----------------------------\n# SQS (main + DLQ)\n# -----------------------------\nresource \"aws_sqs_queue\" \"dlq\" {\n  name                      = \"${var.name_prefix}-dlq\"\n  message_retention_seconds = var.sqs_message_retention_seconds\n\n  # S3 -> SQS requires SSE-SQS (managed) or no encryption. Keep simple.\n  sqs_managed_sse_enabled = true\n}\n\nresource \"aws_sqs_queue\" \"main\" {\n  name                       = \"${var.name_prefix}-queue\"\n  visibility_timeout_seconds = var.sqs_visibility_timeout_seconds\n  message_retention_seconds  = var.sqs_message_retention_seconds\n\n  sqs_managed_sse_enabled = true\n\n  redrive_policy = jsonencode({\n    deadLetterTargetArn = aws_sqs_queue.dlq.arn\n    maxReceiveCount     = var.sqs_max_receive_count\n  })\n}\n\n# Allow ONLY this S3 bucket to send messages to the queue\ndata \"aws_iam_policy_document\" \"sqs_allow_s3\" {\n  statement {\n    sid     = \"AllowS3SendMessage\"\n    effect  = \"Allow\"\n    actions = [\"sqs:SendMessage\"]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"s3.amazonaws.com\"]\n    }\n\n    resources = [aws_sqs_queue.main.arn]\n\n    condition {\n      test     = \"ArnEquals\"\n      variable = \"aws:SourceArn\"\n      values   = [aws_s3_bucket.bucket.arn]\n    }\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"aws:SourceAccount\"\n      values   = [data.aws_caller_identity.current.account_id]\n    }\n  }\n}\n\nresource \"aws_sqs_queue_policy\" \"main\" {\n  queue_url = aws_sqs_queue.main.id\n  policy    = data.aws_iam_policy_document.sqs_allow_s3.json\n}\n\n# -----------------------------\n# S3 bucket\n# -----------------------------\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket        = local.bucket_name\n  force_destroy = true\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"bucket\" {\n  bucket = aws_s3_bucket.bucket.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n\nresource \"aws_s3_bucket_ownership_controls\" \"bucket\" {\n  bucket = aws_s3_bucket.bucket.id\n\n  rule {\n    object_ownership = \"BucketOwnerEnforced\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"bucket\" {\n  bucket = aws_s3_bucket.bucket.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\n# Enforce TLS for all requests\ndata \"aws_iam_policy_document\" \"s3_bucket_policy\" {\n  statement {\n    sid     = \"DenyInsecureTransport\"\n    effect  = \"Deny\"\n    actions = [\"s3:*\"]\n\n    principals {\n      type        = \"*\"\n      identifiers = [\"*\"]\n    }\n\n    resources = [\n      aws_s3_bucket.bucket.arn,\n      \"${aws_s3_bucket.bucket.arn}/*\"\n    ]\n\n    condition {\n      test     = \"Bool\"\n      variable = \"aws:SecureTransport\"\n      values   = [\"false\"]\n    }\n  }\n}\n\nresource \"aws_s3_bucket_policy\" \"bucket\" {\n  bucket = aws_s3_bucket.bucket.id\n  policy = data.aws_iam_policy_document.s3_bucket_policy.json\n}\n\n# S3 -> SQS notification\nresource \"aws_s3_bucket_notification\" \"bucket\" {\n  bucket = aws_s3_bucket.bucket.id\n\n  queue {\n    queue_arn     = aws_sqs_queue.main.arn\n    events        = [\"s3:ObjectCreated:*\"]\n    filter_prefix = var.s3_event_prefix != \"\" ? var.s3_event_prefix : null\n    filter_suffix = var.s3_event_suffix != \"\" ? var.s3_event_suffix : null\n  }\n\n  depends_on = [aws_sqs_queue_policy.main]\n}\n\n# -----------------------------\n# IAM for Lambda\n# -----------------------------\ndata \"aws_iam_policy_document\" \"lambda_assume_role\" {\n  statement {\n    effect  = \"Allow\"\n    actions = [\"sts:AssumeRole\"]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"lambda.amazonaws.com\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"lambda\" {\n  name               = \"${var.name_prefix}-lambda-role\"\n  assume_role_policy = data.aws_iam_policy_document.lambda_assume_role.json\n}\n\ndata \"aws_iam_policy_document\" \"lambda_inline\" {\n  statement {\n    sid     = \"CloudWatchLogs\"\n    effect  = \"Allow\"\n    actions = [\n      \"logs:CreateLogGroup\",\n      \"logs:CreateLogStream\",\n      \"logs:PutLogEvents\"\n    ]\n    resources = [\n      \"arn:${data.aws_partition.current.partition}:logs:${var.aws_region}:${data.aws_caller_identity.current.account_id}:log-group:/aws/lambda/${var.name_prefix}:*\"\n    ]\n  }\n\n  statement {\n    sid     = \"ConsumeFromSQS\"\n    effect  = \"Allow\"\n    actions = [\n      \"sqs:ReceiveMessage\",\n      \"sqs:DeleteMessage\",\n      \"sqs:GetQueueAttributes\",\n      \"sqs:ChangeMessageVisibility\"\n    ]\n    resources = [aws_sqs_queue.main.arn]\n  }\n\n  dynamic \"statement\" {\n    for_each = var.enable_s3_read ? [1] : []\n    content {\n      sid     = \"ReadFromS3\"\n      effect  = \"Allow\"\n      actions = [\n        \"s3:GetObject\"\n      ]\n      resources = [\"${aws_s3_bucket.bucket.arn}/*\"]\n    }\n  }\n}\n\nresource \"aws_iam_role_policy\" \"lambda_inline\" {\n  name   = \"${var.name_prefix}-lambda-inline\"\n  role   = aws_iam_role.lambda.id\n  policy = data.aws_iam_policy_document.lambda_inline.json\n}\n\n# -----------------------------\n# Lambda\n# -----------------------------\nresource \"aws_cloudwatch_log_group\" \"lambda\" {\n  name              = \"/aws/lambda/${var.name_prefix}\"\n  retention_in_days = var.log_retention_days\n}\n\nresource \"aws_lambda_function\" \"this\" {\n  function_name = var.name_prefix\n  role          = aws_iam_role.lambda.arn\n\n  filename         = var.lambda_artifact_path\n  source_code_hash = filebase64sha256(var.lambda_artifact_path)\n\n  runtime = \"provided.al2023\"\n  handler = \"bootstrap\"\n\n  memory_size = var.lambda_memory_size\n  timeout     = var.lambda_timeout_seconds\n\n  environment {\n    variables = {\n      BUCKET_NAME = aws_s3_bucket.bucket.bucket\n      QUEUE_URL   = aws_sqs_queue.main.id\n    }\n  }\n\n  depends_on = [aws_cloudwatch_log_group.lambda]\n}\n\nresource \"aws_lambda_event_source_mapping\" \"sqs\" {\n  event_source_arn = aws_sqs_queue.main.arn\n  function_name    = aws_lambda_function.this.arn\n\n  batch_size                         = var.lambda_sqs_batch_size\n  maximum_batching_window_in_seconds = var.lambda_sqs_maximum_batching_window_seconds\n\n  function_response_types = [\"ReportBatchItemFailures\"]\n}\n\n# -----------------------------\n# Alarms (minimal)\n# -----------------------------\nresource \"aws_cloudwatch_metric_alarm\" \"sqs_backlog\" {\n  alarm_name          = \"${var.name_prefix}-sqs-backlog\"\n  alarm_description   = \"SQS backlog is growing.\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"ApproximateNumberOfMessagesVisible\"\n  namespace           = \"AWS/SQS\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 100\n\n  dimensions = {\n    QueueName = aws_sqs_queue.main.name\n  }\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"dlq_depth\" {\n  alarm_name          = \"${var.name_prefix}-dlq-depth\"\n  alarm_description   = \"Messages are landing in the DLQ.\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"ApproximateNumberOfMessagesVisible\"\n  namespace           = \"AWS/SQS\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 1\n\n  dimensions = {\n    QueueName = aws_sqs_queue.dlq.name\n  }\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_errors\" {\n  alarm_name          = \"${var.name_prefix}-lambda-errors\"\n  alarm_description   = \"Lambda is erroring.\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 1\n\n  dimensions = {\n    FunctionName = aws_lambda_function.this.function_name\n  }\n}\n"
    },
    {
      "name": "outputs.tf",
      "content": "output \"s3_bucket_name\" {\n  description = \"Name of the S3 bucket for CSV uploads.\"\n  value       = aws_s3_bucket.bucket.bucket\n}\n\noutput \"sqs_queue_url\" {\n  description = \"URL of the main SQS queue.\"\n  value       = aws_sqs_queue.main.id\n}\n\noutput \"sqs_queue_arn\" {\n  description = \"ARN of the main SQS queue.\"\n  value       = aws_sqs_queue.main.arn\n}\n\noutput \"sqs_dlq_url\" {\n  description = \"URL of the dead-letter queue.\"\n  value       = aws_sqs_queue.dlq.id\n}\n\noutput \"lambda_function_name\" {\n  description = \"Lambda function name.\"\n  value       = aws_lambda_function.this.function_name\n}\n"
    },
    {
      "name": "terraform.tfvars",
      "content": "aws_region = \"ap-southeast-1\"\n\n# Keep the default artifact path relative to ai_tf/\nlambda_artifact_path = \"../lambda/build/handler.zip\"\n\n# Optional: grant Lambda permission to read from S3 if your handler needs it\nenable_s3_read = false\n\n# Only notify on CSV uploads\ns3_event_suffix = \".csv\"\n"
    }
  ]
}