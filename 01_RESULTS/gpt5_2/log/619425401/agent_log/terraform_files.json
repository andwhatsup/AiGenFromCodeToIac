{
  "files": [
    {
      "name": "versions.tf",
      "content": "terraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 6.28\"\n    }\n    archive = {\n      source  = \"hashicorp/archive\"\n      version = \"~> 2.7\"\n    }\n  }\n}\n"
    },
    {
      "name": "provider.tf",
      "content": "provider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Project     = var.project_name\n      Environment = var.environment\n      ManagedBy   = \"terraform\"\n    }\n  }\n}\n"
    },
    {
      "name": "variables.tf",
      "content": "variable \"aws_region\" {\n  description = \"AWS region to deploy into.\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"project_name\" {\n  description = \"Project/name prefix used for resource naming.\"\n  type        = string\n  default     = \"lambda-delta-optimize\"\n}\n\nvariable \"environment\" {\n  description = \"Environment name (e.g., dev, staging, prod).\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"lambda_name\" {\n  description = \"Lambda function name.\"\n  type        = string\n  default     = \"lambda-delta-optimize\"\n}\n\nvariable \"lambda_zip_path\" {\n  description = \"Path to the Lambda deployment ZIP produced by cargo-lambda (bootstrap.zip).\"\n  type        = string\n  default     = \"../target/lambda/lambda-delta-optimize/bootstrap.zip\"\n}\n\nvariable \"datalake_bucket_name\" {\n  description = \"S3 bucket name that stores the Delta Lake table.\"\n  type        = string\n}\n\nvariable \"datalake_prefix\" {\n  description = \"S3 prefix (folder) under the bucket where the Delta Lake table lives (no leading slash).\"\n  type        = string\n  default     = \"databases/bronze/http\"\n}\n\nvariable \"optimize_ds\" {\n  description = \"Value for OPTIMIZE_DS env var. Use 'yesterday' to optimize yesterday's ds partition.\"\n  type        = string\n  default     = \"yesterday\"\n}\n\nvariable \"schedule_expression\" {\n  description = \"EventBridge schedule expression (rate() or cron()).\"\n  type        = string\n  default     = \"rate(1 day)\"\n}\n\nvariable \"lambda_memory_size\" {\n  description = \"Lambda memory size in MB. Delta/Parquet workloads often need > 1024MB.\"\n  type        = number\n  default     = 1024\n}\n\nvariable \"lambda_timeout_seconds\" {\n  description = \"Lambda timeout in seconds.\"\n  type        = number\n  default     = 900\n}\n\nvariable \"lambda_reserved_concurrency\" {\n  description = \"Reserved concurrency for the Lambda. Set to 1 to avoid concurrent runs. Use -1 for unreserved.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"log_retention_days\" {\n  description = \"CloudWatch log retention in days.\"\n  type        = number\n  default     = 30\n}\n\nvariable \"create_datalake_bucket\" {\n  description = \"Whether Terraform should create the S3 bucket. If false, bucket must already exist.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"s3_force_destroy\" {\n  description = \"If creating the bucket, whether to allow force_destroy.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enable_eventbridge_dlq\" {\n  description = \"Whether to create an SQS DLQ for failed EventBridge->Lambda invocations.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"alarm_sns_topic_arn\" {\n  description = \"Optional SNS topic ARN to notify alarms. If null, alarms are created without actions.\"\n  type        = string\n  default     = null\n}\n"
    },
    {
      "name": "main.tf",
      "content": "data \"aws_caller_identity\" \"current\" {}\n\ndata \"aws_partition\" \"current\" {}\n\ndata \"aws_region\" \"current\" {}\n\nlocals {\n  name_prefix = \"${var.project_name}-${var.environment}\"\n\n  datalake_location = \"s3://${var.datalake_bucket_name}/${var.datalake_prefix}\"\n\n  alarm_actions = var.alarm_sns_topic_arn == null ? [] : [var.alarm_sns_topic_arn]\n}\n\n# -----------------------------\n# S3 (optional creation)\n# -----------------------------\nresource \"aws_s3_bucket\" \"datalake\" {\n  count         = var.create_datalake_bucket ? 1 : 0\n  bucket        = var.datalake_bucket_name\n  force_destroy = var.s3_force_destroy\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"datalake\" {\n  count  = var.create_datalake_bucket ? 1 : 0\n  bucket = aws_s3_bucket.datalake[0].id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n\nresource \"aws_s3_bucket_versioning\" \"datalake\" {\n  count  = var.create_datalake_bucket ? 1 : 0\n  bucket = aws_s3_bucket.datalake[0].id\n\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"datalake\" {\n  count  = var.create_datalake_bucket ? 1 : 0\n  bucket = aws_s3_bucket.datalake[0].id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\nresource \"aws_s3_bucket_policy\" \"datalake_tls\" {\n  count  = var.create_datalake_bucket ? 1 : 0\n  bucket = aws_s3_bucket.datalake[0].id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid       = \"DenyInsecureTransport\"\n        Effect    = \"Deny\"\n        Principal = \"*\"\n        Action    = \"s3:*\"\n        Resource = [\n          aws_s3_bucket.datalake[0].arn,\n          \"${aws_s3_bucket.datalake[0].arn}/*\"\n        ]\n        Condition = {\n          Bool = {\n            \"aws:SecureTransport\" = \"false\"\n          }\n        }\n      }\n    ]\n  })\n}\n\n# -----------------------------\n# DynamoDB lock table\n# -----------------------------\nresource \"aws_dynamodb_table\" \"delta_lock\" {\n  name         = \"${local.name_prefix}-delta-lock\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"key\"\n\n  attribute {\n    name = \"key\"\n    type = \"S\"\n  }\n\n  point_in_time_recovery {\n    enabled = true\n  }\n}\n\n# -----------------------------\n# IAM for Lambda\n# -----------------------------\ndata \"aws_iam_policy_document\" \"lambda_assume\" {\n  statement {\n    effect = \"Allow\"\n    actions = [\n      \"sts:AssumeRole\"\n    ]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"lambda.amazonaws.com\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"lambda\" {\n  name               = \"${local.name_prefix}-lambda-role\"\n  assume_role_policy = data.aws_iam_policy_document.lambda_assume.json\n}\n\n# Least-privilege S3 access to the bucket/prefix\n# Note: Delta Lake needs list on bucket and read/write on objects.\n# We scope ListBucket to the prefix.\n\ndata \"aws_iam_policy_document\" \"lambda_policy\" {\n  statement {\n    sid     = \"CloudWatchLogs\"\n    effect  = \"Allow\"\n    actions = [\n      \"logs:CreateLogGroup\",\n      \"logs:CreateLogStream\",\n      \"logs:PutLogEvents\"\n    ]\n    resources = [\n      \"arn:${data.aws_partition.current.partition}:logs:${data.aws_region.current.region}:${data.aws_caller_identity.current.account_id}:log-group:/aws/lambda/${var.lambda_name}:*\"\n    ]\n  }\n\n  statement {\n    sid    = \"S3ListPrefix\"\n    effect = \"Allow\"\n    actions = [\n      \"s3:ListBucket\"\n    ]\n    resources = [\n      \"arn:${data.aws_partition.current.partition}:s3:::${var.datalake_bucket_name}\"\n    ]\n    condition {\n      test     = \"StringLike\"\n      variable = \"s3:prefix\"\n      values   = [\"${var.datalake_prefix}/*\", var.datalake_prefix]\n    }\n  }\n\n  statement {\n    sid    = \"S3ObjectRW\"\n    effect = \"Allow\"\n    actions = [\n      \"s3:GetObject\",\n      \"s3:PutObject\",\n      \"s3:DeleteObject\",\n      \"s3:AbortMultipartUpload\",\n      \"s3:ListMultipartUploadParts\"\n    ]\n    resources = [\n      \"arn:${data.aws_partition.current.partition}:s3:::${var.datalake_bucket_name}/${var.datalake_prefix}/*\"\n    ]\n  }\n\n  statement {\n    sid    = \"DynamoDBLockTable\"\n    effect = \"Allow\"\n    actions = [\n      \"dynamodb:GetItem\",\n      \"dynamodb:PutItem\",\n      \"dynamodb:UpdateItem\",\n      \"dynamodb:DeleteItem\",\n      \"dynamodb:DescribeTable\",\n      \"dynamodb:Query\",\n      \"dynamodb:Scan\"\n    ]\n    resources = [\n      aws_dynamodb_table.delta_lock.arn\n    ]\n  }\n}\n\nresource \"aws_iam_role_policy\" \"lambda_inline\" {\n  name   = \"${local.name_prefix}-lambda-inline\"\n  role   = aws_iam_role.lambda.id\n  policy = data.aws_iam_policy_document.lambda_policy.json\n}\n\n# -----------------------------\n# CloudWatch Log Group\n# -----------------------------\nresource \"aws_cloudwatch_log_group\" \"lambda\" {\n  name              = \"/aws/lambda/${var.lambda_name}\"\n  retention_in_days = var.log_retention_days\n}\n\n# -----------------------------\n# Lambda function\n# -----------------------------\nresource \"aws_lambda_function\" \"this\" {\n  function_name = var.lambda_name\n  role          = aws_iam_role.lambda.arn\n\n  runtime = \"provided.al2\"\n  handler = \"bootstrap\"\n\n  filename         = var.lambda_zip_path\n  source_code_hash = filebase64sha256(var.lambda_zip_path)\n\n  memory_size = var.lambda_memory_size\n  timeout     = var.lambda_timeout_seconds\n\n  reserved_concurrent_executions = var.lambda_reserved_concurrency\n\n  environment {\n    variables = {\n      DATALAKE_LOCATION        = local.datalake_location\n      AWS_S3_LOCKING_PROVIDER  = \"dynamodb\"\n      DYNAMODB_LOCK_TABLE_NAME = aws_dynamodb_table.delta_lock.name\n      OPTIMIZE_DS              = var.optimize_ds\n      RUST_LOG                 = \"info\"\n    }\n  }\n\n  depends_on = [aws_cloudwatch_log_group.lambda]\n}\n\n# -----------------------------\n# EventBridge schedule + permissions\n# -----------------------------\nresource \"aws_cloudwatch_event_rule\" \"schedule\" {\n  name                = \"${local.name_prefix}-schedule\"\n  description         = \"Scheduled trigger for ${var.lambda_name}\"\n  schedule_expression = var.schedule_expression\n}\n\nresource \"aws_lambda_permission\" \"allow_eventbridge\" {\n  statement_id  = \"AllowExecutionFromEventBridge\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.this.function_name\n  principal     = \"events.amazonaws.com\"\n  source_arn    = aws_cloudwatch_event_rule.schedule.arn\n}\n\n# Optional DLQ for failed invocations\nresource \"aws_sqs_queue\" \"eventbridge_dlq\" {\n  count = var.enable_eventbridge_dlq ? 1 : 0\n\n  name                      = \"${local.name_prefix}-eventbridge-dlq\"\n  message_retention_seconds = 1209600\n\n  kms_master_key_id = \"alias/aws/sqs\"\n}\n\nresource \"aws_cloudwatch_event_target\" \"lambda\" {\n  rule      = aws_cloudwatch_event_rule.schedule.name\n  target_id = \"lambda\"\n  arn       = aws_lambda_function.this.arn\n\n  retry_policy {\n    maximum_event_age_in_seconds = 3600\n    maximum_retry_attempts       = 2\n  }\n\n  dynamic \"dead_letter_config\" {\n    for_each = var.enable_eventbridge_dlq ? [1] : []\n    content {\n      arn = aws_sqs_queue.eventbridge_dlq[0].arn\n    }\n  }\n\n  depends_on = [aws_lambda_permission.allow_eventbridge]\n}\n\n# -----------------------------\n# Alarms (basic)\n# -----------------------------\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_errors\" {\n  alarm_name          = \"${local.name_prefix}-lambda-errors\"\n  alarm_description   = \"Lambda errors > 0\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = 300\n  statistic           = \"Sum\"\n  threshold           = 0\n\n  dimensions = {\n    FunctionName = aws_lambda_function.this.function_name\n  }\n\n  alarm_actions = local.alarm_actions\n  ok_actions    = local.alarm_actions\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_throttles\" {\n  alarm_name          = \"${local.name_prefix}-lambda-throttles\"\n  alarm_description   = \"Lambda throttles > 0\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"Throttles\"\n  namespace           = \"AWS/Lambda\"\n  period              = 300\n  statistic           = \"Sum\"\n  threshold           = 0\n\n  dimensions = {\n    FunctionName = aws_lambda_function.this.function_name\n  }\n\n  alarm_actions = local.alarm_actions\n  ok_actions    = local.alarm_actions\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"eventbridge_failed_invocations\" {\n  alarm_name          = \"${local.name_prefix}-eventbridge-failed-invocations\"\n  alarm_description   = \"EventBridge failed invocations > 0\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"FailedInvocations\"\n  namespace           = \"AWS/Events\"\n  period              = 300\n  statistic           = \"Sum\"\n  threshold           = 0\n\n  dimensions = {\n    RuleName = aws_cloudwatch_event_rule.schedule.name\n  }\n\n  alarm_actions = local.alarm_actions\n  ok_actions    = local.alarm_actions\n}\n"
    },
    {
      "name": "outputs.tf",
      "content": "output \"lambda_function_name\" {\n  description = \"Deployed Lambda function name.\"\n  value       = aws_lambda_function.this.function_name\n}\n\noutput \"eventbridge_rule_name\" {\n  description = \"EventBridge schedule rule name.\"\n  value       = aws_cloudwatch_event_rule.schedule.name\n}\n\noutput \"dynamodb_lock_table_name\" {\n  description = \"DynamoDB lock table name used by delta-rs.\"\n  value       = aws_dynamodb_table.delta_lock.name\n}\n\noutput \"datalake_location\" {\n  description = \"S3 URL passed to the Lambda as DATALAKE_LOCATION.\"\n  value       = local.datalake_location\n}\n\noutput \"eventbridge_dlq_url\" {\n  description = \"SQS DLQ URL (if enabled).\"\n  value       = var.enable_eventbridge_dlq ? aws_sqs_queue.eventbridge_dlq[0].url : null\n}\n"
    },
    {
      "name": "terraform.tfvars.example",
      "content": "aws_region   = \"us-east-1\"\nenvironment  = \"dev\"\nproject_name = \"lambda-delta-optimize\"\n\n# Lambda artifact produced by:\n#   cargo lambda build --release --output-format zip\nlambda_zip_path = \"../target/lambda/lambda-delta-optimize/bootstrap.zip\"\n\n# Existing bucket that contains your Delta table\ncreate_datalake_bucket = false\ndatalake_bucket_name   = \"my-bucket-name\"\ndatalake_prefix        = \"databases/bronze/http\"\n\n# Optimize only yesterday's ds partition\noptimize_ds = \"yesterday\"\n\n# Run daily\nschedule_expression = \"rate(1 day)\"\n\nlambda_memory_size         = 2048\nlambda_timeout_seconds     = 900\nlambda_reserved_concurrency = 1\n\nlog_retention_days = 30\n\n# Optional: send alarms to an SNS topic\n# alarm_sns_topic_arn = \"arn:aws:sns:us-east-1:123456789012:my-topic\"\n"
    }
  ]
}