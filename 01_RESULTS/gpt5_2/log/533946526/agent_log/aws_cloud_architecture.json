{
  "aws_architecture_description": "## Proposed high-level AWS architecture (RKE-provisioned Kubernetes on AWS)\n\n### 1) Core idea\nThis repo is an **ops automation project** (Ansible + shell, with an optional Terraform step) that is executed from a “workstation” to:\n- provision infrastructure (expected via Terraform), and\n- provision/configure a **Kubernetes cluster using Rancher RKE** (i.e., RKE on EC2 instances, not EKS).\n\nOn AWS, the cleanest mapping is:\n- **Kubernetes nodes = EC2 instances** (separate control-plane and worker node groups)\n- **Workstation = CI runner or a managed “ops host”** that runs the scripts/Ansible and talks to the cluster\n- **State/artifacts/secrets/logs = managed AWS services**\n\n### 2) Networking layout (VPC)\n- Create a dedicated **VPC** spanning **2–3 Availability Zones**.\n- **Public subnets**: only for internet-facing load balancers and (optionally) a tightly controlled bastion.\n- **Private subnets**: for all Kubernetes control-plane and worker nodes.\n- **NAT Gateways** (one per AZ for HA, or one to reduce cost) so private instances can reach the internet for OS/package/container image pulls.\n- **VPC endpoints** to reduce NAT usage and keep traffic private where possible:\n  - S3 Gateway endpoint\n  - Interface endpoints for ECR (api + dkr), CloudWatch Logs, SSM, EC2, STS (as needed)\n\n### 3) Compute: RKE cluster on EC2\n- Provision EC2 instances for:\n  - **Control plane / etcd nodes** (recommended 3 nodes across AZs)\n  - **Worker nodes** (Auto Scaling Group for elasticity)\n- Use a hardened base image (e.g., CIS-aligned AMI if required) and bootstrap via cloud-init/user-data or Ansible.\n- Use **instance profiles (IAM roles)** for nodes to access only what they need (e.g., pull from ECR, write logs to CloudWatch).\n\n**Container runtime**: RKE commonly uses containerd or Docker depending on configuration; ensure security groups and OS config match your chosen runtime.\n\n### 4) Cluster access & “workstation” execution model\nBecause your inventory targets `localhost`, you need a place to run the automation from:\n\n**Option A (recommended): GitHub Actions runner on AWS**\n- Deploy a **self-hosted GitHub Actions runner** on an EC2 instance in a private subnet.\n- Runner executes `bin/01_install_cli_tools.sh` and `bin/02_create_cluster.sh`.\n- Runner reaches the cluster over private networking.\n\n**Option B: AWS Systems Manager (SSM) “ops host”**\n- Maintain an EC2 “ops host” managed via SSM Session Manager (no inbound SSH).\n- Operators trigger runs manually or via SSM Automation.\n\nIn both options:\n- Store sensitive material (SSH keys for node access, kubeconfig, tokens) in **AWS Secrets Manager**.\n- Use **KMS** to encrypt secrets and any state buckets.\n\n### 5) Ingress / Load balancing\nRKE itself doesn’t provide AWS load balancers automatically unless you install the AWS cloud provider / CCM and a compatible ingress controller.\n\nA pragmatic AWS approach:\n- Install an ingress controller (commonly **NGINX Ingress** or similar) via Helm.\n- Expose it using an **AWS Network Load Balancer (NLB)** or **Application Load Balancer (ALB)**.\n  - If you want ALB-native ingress, install the **AWS Load Balancer Controller** (requires IAM OIDC integration; with RKE you’ll manage IAM creds differently than EKS).\n\n### 6) Storage\n- **EBS** for node root volumes and (optionally) Kubernetes persistent volumes via a CSI driver.\n- **S3** for:\n  - Terraform state (if you add Terraform back into the repo)\n  - cluster backups/artifacts (optional)\n- If you need shared POSIX storage, add **EFS** and mount via CSI.\n\n### 7) Container images\n- Use **Amazon ECR** as the container registry for workloads you deploy to the cluster.\n- Nodes authenticate to ECR via instance profile permissions.\n\n### 8) Observability & security\n- **CloudWatch Logs** for system logs (via CloudWatch Agent) and optionally container logs (via Fluent Bit/OTel collector).\n- **CloudWatch metrics/alarms** for EC2, ASG, load balancers, NAT, etc.\n- **CloudTrail** enabled for audit.\n- **AWS Config** (optional but recommended in regulated environments).\n- **Security Hub / GuardDuty** (optional but recommended for baseline threat detection).\n\n### 9) CI security scanning (tfsec)\nYour repo already runs tfsec in GitHub Actions. On AWS, complement this with:\n- S3 + DynamoDB for Terraform state locking (when Terraform code exists)\n- IAM least privilege for the runner role\n\n### 10) Deployment flow (end-to-end)\n1. GitHub Actions (self-hosted runner on AWS) checks out repo.\n2. Runner executes `bin/01_install_cli_tools.sh` to install Poetry/Ansible and required CLIs.\n3. Runner executes `bin/02_create_cluster.sh` to run Terraform (once Terraform code is present) to create VPC/EC2/ASGs/etc.\n4. Runner uses RKE + kubectl/helm/flux to bootstrap the cluster and deploy platform components.\n5. Workloads are deployed via Helm/Flux; images pulled from ECR; ingress exposed via ALB/NLB.\n\n---\n\n## Notes / assumptions\n- This design intentionally keeps things **lean** and aligned to your repo’s intent (RKE on nodes). If you are open to changing the platform, **EKS** would reduce operational burden substantially, but it’s a different architecture than “RKE-provisioned cluster.”\n- Because the Terraform directory is missing in the snapshot, the infrastructure layer below is the recommended target for that Terraform.\n",
  "aws_resources": [
    "Networking (VPC foundation)",
    "Amazon VPC",
    "VPC CIDR block(s)",
    "Public Subnets (2–3 AZs)",
    "Private Subnets (2–3 AZs)",
    "Internet Gateway (IGW)",
    "NAT Gateway(s)",
    "Elastic IP(s) for NAT",
    "Route Tables (public/private) + routes",
    "VPC Endpoints: S3 Gateway Endpoint",
    "VPC Endpoints (Interface): ECR (api, dkr)",
    "VPC Endpoints (Interface): CloudWatch Logs",
    "VPC Endpoints (Interface): SSM, EC2 Messages, SSM Messages (for Session Manager)",
    "VPC Endpoints (Interface): STS (as needed)",
    "Security",
    "Security Groups (control plane nodes)",
    "Security Groups (worker nodes)",
    "Security Groups (load balancer)",
    "Network ACLs (optional; if you enforce beyond SGs)",
    "AWS KMS Key(s) (for S3/Secrets/EBS encryption)",
    "Compute (Kubernetes on EC2)",
    "EC2 Launch Template(s) (control plane)",
    "EC2 Launch Template(s) (workers)",
    "EC2 Auto Scaling Group (workers)",
    "EC2 Instances (control plane; fixed-size group or ASG)",
    "EBS Volumes (root + data as needed)",
    "IAM instance profile(s) for Kubernetes nodes",
    "IAM Roles + Policies (least privilege for nodes)",
    "Cluster access / Ops execution",
    "EC2 Instance for self-hosted GitHub Actions runner (or Ops host)",
    "IAM Role for runner/Ops host (provisioning permissions)",
    "AWS Systems Manager (SSM) managed instance configuration",
    "Secrets & configuration",
    "AWS Secrets Manager (SSH keys, kubeconfig, tokens)",
    "AWS Systems Manager Parameter Store (non-secret config; optional)",
    "Container registry",
    "Amazon ECR Repositories",
    "ECR lifecycle policies",
    "Ingress / traffic",
    "Elastic Load Balancing: Network Load Balancer (NLB) or Application Load Balancer (ALB)",
    "Target Groups",
    "Listeners (HTTP/HTTPS)",
    "ACM Certificate(s) (for TLS on ALB/NLB where applicable)",
    "Route 53 Hosted Zone + DNS records (if you manage DNS in AWS)",
    "State & artifacts (if Terraform is used)",
    "Amazon S3 Bucket for Terraform state",
    "S3 Bucket versioning + default encryption",
    "DynamoDB table for Terraform state locking",
    "Logging, monitoring, audit",
    "Amazon CloudWatch Log Groups",
    "CloudWatch Agent / log shipping configuration (deployed to nodes)",
    "CloudWatch Alarms (EC2/ASG/ELB/NAT)",
    "AWS CloudTrail (org or account trail)",
    "AWS Config (optional)",
    "Security Hub (optional)",
    "GuardDuty (optional)"
  ]
}