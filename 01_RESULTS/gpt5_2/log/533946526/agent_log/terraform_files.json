{
  "files": [
    {
      "name": "versions.tf",
      "content": "terraform {\n  required_version = \">= 1.6.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n}\n"
    },
    {
      "name": "provider.tf",
      "content": "provider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = var.tags\n  }\n}\n"
    },
    {
      "name": "variables.tf",
      "content": "variable \"aws_region\" {\n  description = \"AWS region to deploy into.\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"name\" {\n  description = \"Name prefix for all resources.\"\n  type        = string\n  default     = \"rke\"\n}\n\nvariable \"tags\" {\n  description = \"Tags applied to all resources via provider default_tags.\"\n  type        = map(string)\n  default = {\n    Project = \"rke-on-ec2\"\n    Managed = \"terraform\"\n  }\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR.\"\n  type        = string\n  default     = \"10.20.0.0/16\"\n}\n\nvariable \"az_count\" {\n  description = \"Number of AZs to use (2 or 3).\"\n  type        = number\n  default     = 2\n  validation {\n    condition     = var.az_count >= 2 && var.az_count <= 3\n    error_message = \"az_count must be 2 or 3.\"\n  }\n}\n\nvariable \"enable_nat_gateway\" {\n  description = \"Whether to create NAT gateways for private subnets.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"single_nat_gateway\" {\n  description = \"If true, create a single NAT gateway (lower cost, less HA).\"\n  type        = bool\n  default     = true\n}\n\nvariable \"enable_vpc_endpoints\" {\n  description = \"Create VPC endpoints for S3, ECR, CloudWatch Logs, and SSM.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"ssh_key_name\" {\n  description = \"Optional EC2 key pair name to allow SSH access (prefer SSM).\"\n  type        = string\n  default     = null\n}\n\nvariable \"allowed_ssh_cidrs\" {\n  description = \"CIDRs allowed to SSH to nodes (only used if ssh_key_name is set).\"\n  type        = list(string)\n  default     = []\n}\n\nvariable \"control_plane_instance_type\" {\n  description = \"Instance type for control plane nodes.\"\n  type        = string\n  default     = \"t3.medium\"\n}\n\nvariable \"control_plane_count\" {\n  description = \"Number of control plane nodes (recommended 3 for HA).\"\n  type        = number\n  default     = 1\n}\n\nvariable \"worker_instance_type\" {\n  description = \"Instance type for worker nodes.\"\n  type        = string\n  default     = \"t3.large\"\n}\n\nvariable \"worker_min_size\" {\n  description = \"ASG min size for workers.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"worker_max_size\" {\n  description = \"ASG max size for workers.\"\n  type        = number\n  default     = 3\n}\n\nvariable \"worker_desired_size\" {\n  description = \"ASG desired size for workers.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"root_volume_size_gb\" {\n  description = \"Root EBS volume size for nodes.\"\n  type        = number\n  default     = 50\n}\n\nvariable \"enable_ops_host\" {\n  description = \"Create an ops host (can be used as a self-hosted GitHub Actions runner / Ansible execution host).\"\n  type        = bool\n  default     = true\n}\n\nvariable \"ops_instance_type\" {\n  description = \"Instance type for ops host.\"\n  type        = string\n  default     = \"t3.small\"\n}\n\nvariable \"create_state_backend\" {\n  description = \"Create S3 bucket + DynamoDB table for Terraform state (bucket name must be globally unique).\"\n  type        = bool\n  default     = false\n}\n\nvariable \"state_bucket_name\" {\n  description = \"S3 bucket name for Terraform state (required if create_state_backend=true).\"\n  type        = string\n  default     = null\n}\n"
    },
    {
      "name": "main.tf",
      "content": "data \"aws_availability_zones\" \"available\" {\n  state = \"available\"\n}\n\nlocals {\n  azs = slice(data.aws_availability_zones.available.names, 0, var.az_count)\n\n  public_subnets  = [for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i)]\n  private_subnets = [for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 10)]\n\n  cluster_name = var.name\n}\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~> 5.0\"\n\n  name = \"${var.name}-vpc\"\n  cidr = var.vpc_cidr\n\n  azs             = local.azs\n  public_subnets  = local.public_subnets\n  private_subnets = local.private_subnets\n\n  enable_nat_gateway = var.enable_nat_gateway\n  single_nat_gateway = var.single_nat_gateway\n\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  public_subnet_tags = {\n    \"kubernetes.io/role/elb\" = \"1\"\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/role/internal-elb\" = \"1\"\n  }\n}\n\n# --- KMS key for EBS encryption and general use (optional but good baseline)\nresource \"aws_kms_key\" \"main\" {\n  description             = \"${var.name} key for EBS and general encryption\"\n  deletion_window_in_days = 7\n  enable_key_rotation     = true\n}\n\nresource \"aws_kms_alias\" \"main\" {\n  name          = \"alias/${var.name}-main\"\n  target_key_id = aws_kms_key.main.key_id\n}\n\n# --- AMI\n# Use latest Amazon Linux 2023. You can swap to Ubuntu if your RKE playbooks expect it.\ndata \"aws_ami\" \"al2023\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"al2023-ami-*-x86_64\"]\n  }\n\n  filter {\n    name   = \"architecture\"\n    values = [\"x86_64\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\n# --- Security groups\nresource \"aws_security_group\" \"control_plane\" {\n  name        = \"${var.name}-control-plane\"\n  description = \"RKE control plane security group\"\n  vpc_id      = module.vpc.vpc_id\n\n  # Kubernetes API (typically 6443) - allow from within VPC and ops host\n  ingress {\n    description = \"Kubernetes API from VPC\"\n    from_port   = 6443\n    to_port     = 6443\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # etcd peer/client (RKE uses 2379-2380)\n  ingress {\n    description = \"etcd from VPC\"\n    from_port   = 2379\n    to_port     = 2380\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # kubelet\n  ingress {\n    description = \"kubelet from VPC\"\n    from_port   = 10250\n    to_port     = 10250\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # NodePort range (if used)\n  ingress {\n    description = \"NodePort from VPC\"\n    from_port   = 30000\n    to_port     = 32767\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # Optional SSH\n  dynamic \"ingress\" {\n    for_each = var.ssh_key_name != null && length(var.allowed_ssh_cidrs) > 0 ? [1] : []\n    content {\n      description = \"SSH\"\n      from_port   = 22\n      to_port     = 22\n      protocol    = \"tcp\"\n      cidr_blocks = var.allowed_ssh_cidrs\n    }\n  }\n\n  egress {\n    description = \"All egress\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_security_group\" \"workers\" {\n  name        = \"${var.name}-workers\"\n  description = \"RKE worker security group\"\n  vpc_id      = module.vpc.vpc_id\n\n  # kubelet\n  ingress {\n    description = \"kubelet from VPC\"\n    from_port   = 10250\n    to_port     = 10250\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # NodePort range\n  ingress {\n    description = \"NodePort from VPC\"\n    from_port   = 30000\n    to_port     = 32767\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  # Optional SSH\n  dynamic \"ingress\" {\n    for_each = var.ssh_key_name != null && length(var.allowed_ssh_cidrs) > 0 ? [1] : []\n    content {\n      description = \"SSH\"\n      from_port   = 22\n      to_port     = 22\n      protocol    = \"tcp\"\n      cidr_blocks = var.allowed_ssh_cidrs\n    }\n  }\n\n  egress {\n    description = \"All egress\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n# Allow control-plane <-> workers traffic broadly within VPC (simplified baseline)\nresource \"aws_security_group_rule\" \"cp_to_workers_all\" {\n  type                     = \"ingress\"\n  security_group_id        = aws_security_group.workers.id\n  from_port                = 0\n  to_port                  = 0\n  protocol                 = \"-1\"\n  source_security_group_id = aws_security_group.control_plane.id\n  description              = \"All from control plane\"\n}\n\nresource \"aws_security_group_rule\" \"workers_to_cp_all\" {\n  type                     = \"ingress\"\n  security_group_id        = aws_security_group.control_plane.id\n  from_port                = 0\n  to_port                  = 0\n  protocol                 = \"-1\"\n  source_security_group_id = aws_security_group.workers.id\n  description              = \"All from workers\"\n}\n\n# --- IAM for nodes (SSM + ECR read-only)\nresource \"aws_iam_role\" \"node\" {\n  name = \"${var.name}-node-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"ec2.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"node_ssm\" {\n  role       = aws_iam_role.node.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n}\n\nresource \"aws_iam_role_policy_attachment\" \"node_ecr\" {\n  role       = aws_iam_role.node.name\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n}\n\nresource \"aws_iam_instance_profile\" \"node\" {\n  name = \"${var.name}-node-profile\"\n  role = aws_iam_role.node.name\n}\n\n# --- Launch templates\nresource \"aws_launch_template\" \"control_plane\" {\n  name_prefix   = \"${var.name}-cp-\"\n  image_id      = data.aws_ami.al2023.id\n  instance_type = var.control_plane_instance_type\n\n  key_name = var.ssh_key_name\n\n  vpc_security_group_ids = [aws_security_group.control_plane.id]\n\n  iam_instance_profile {\n    name = aws_iam_instance_profile.node.name\n  }\n\n  block_device_mappings {\n    device_name = \"/dev/xvda\"\n\n    ebs {\n      volume_size           = var.root_volume_size_gb\n      volume_type           = \"gp3\"\n      encrypted             = true\n      kms_key_id            = aws_kms_key.main.arn\n      delete_on_termination = true\n    }\n  }\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  user_data = base64encode(<<-EOT\n    #!/bin/bash\n    set -euo pipefail\n    dnf -y update\n    # SSM agent is preinstalled on AL2023; ensure it's enabled\n    systemctl enable --now amazon-ssm-agent || true\n  EOT\n  )\n\n  tag_specifications {\n    resource_type = \"instance\"\n    tags = {\n      Name = \"${var.name}-control-plane\"\n      Role = \"control-plane\"\n    }\n  }\n}\n\nresource \"aws_launch_template\" \"workers\" {\n  name_prefix   = \"${var.name}-worker-\"\n  image_id      = data.aws_ami.al2023.id\n  instance_type = var.worker_instance_type\n\n  key_name = var.ssh_key_name\n\n  vpc_security_group_ids = [aws_security_group.workers.id]\n\n  iam_instance_profile {\n    name = aws_iam_instance_profile.node.name\n  }\n\n  block_device_mappings {\n    device_name = \"/dev/xvda\"\n\n    ebs {\n      volume_size           = var.root_volume_size_gb\n      volume_type           = \"gp3\"\n      encrypted             = true\n      kms_key_id            = aws_kms_key.main.arn\n      delete_on_termination = true\n    }\n  }\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  user_data = base64encode(<<-EOT\n    #!/bin/bash\n    set -euo pipefail\n    dnf -y update\n    systemctl enable --now amazon-ssm-agent || true\n  EOT\n  )\n\n  tag_specifications {\n    resource_type = \"instance\"\n    tags = {\n      Name = \"${var.name}-worker\"\n      Role = \"worker\"\n    }\n  }\n}\n\n# --- Control plane instances (fixed count)\nresource \"aws_instance\" \"control_plane\" {\n  count = var.control_plane_count\n\n  ami           = data.aws_ami.al2023.id\n  instance_type = var.control_plane_instance_type\n\n  subnet_id              = element(module.vpc.private_subnets, count.index % length(module.vpc.private_subnets))\n  vpc_security_group_ids = [aws_security_group.control_plane.id]\n  key_name               = var.ssh_key_name\n\n  iam_instance_profile = aws_iam_instance_profile.node.name\n\n  root_block_device {\n    volume_size           = var.root_volume_size_gb\n    volume_type           = \"gp3\"\n    encrypted             = true\n    kms_key_id            = aws_kms_key.main.arn\n    delete_on_termination = true\n  }\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  user_data = <<-EOT\n    #!/bin/bash\n    set -euo pipefail\n    dnf -y update\n    systemctl enable --now amazon-ssm-agent || true\n  EOT\n\n  tags = {\n    Name = \"${var.name}-cp-${count.index}\"\n    Role = \"control-plane\"\n  }\n}\n\n# --- Worker ASG\nresource \"aws_autoscaling_group\" \"workers\" {\n  name                      = \"${var.name}-workers\"\n  max_size                  = var.worker_max_size\n  min_size                  = var.worker_min_size\n  desired_capacity          = var.worker_desired_size\n  vpc_zone_identifier       = module.vpc.private_subnets\n  health_check_type         = \"EC2\"\n  health_check_grace_period = 300\n\n  launch_template {\n    id      = aws_launch_template.workers.id\n    version = \"$Latest\"\n  }\n\n  tag {\n    key                 = \"Name\"\n    value               = \"${var.name}-worker\"\n    propagate_at_launch = true\n  }\n\n  tag {\n    key                 = \"Role\"\n    value               = \"worker\"\n    propagate_at_launch = true\n  }\n}\n\n# --- Ops host (optional)\nresource \"aws_security_group\" \"ops\" {\n  count       = var.enable_ops_host ? 1 : 0\n  name        = \"${var.name}-ops\"\n  description = \"Ops host security group\"\n  vpc_id      = module.vpc.vpc_id\n\n  # Optional SSH to ops host\n  dynamic \"ingress\" {\n    for_each = var.ssh_key_name != null && length(var.allowed_ssh_cidrs) > 0 ? [1] : []\n    content {\n      description = \"SSH\"\n      from_port   = 22\n      to_port     = 22\n      protocol    = \"tcp\"\n      cidr_blocks = var.allowed_ssh_cidrs\n    }\n  }\n\n  egress {\n    description = \"All egress\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_instance\" \"ops\" {\n  count = var.enable_ops_host ? 1 : 0\n\n  ami           = data.aws_ami.al2023.id\n  instance_type = var.ops_instance_type\n\n  subnet_id              = element(module.vpc.private_subnets, 0)\n  vpc_security_group_ids = [aws_security_group.ops[0].id]\n  key_name               = var.ssh_key_name\n\n  iam_instance_profile = aws_iam_instance_profile.node.name\n\n  metadata_options {\n    http_endpoint               = \"enabled\"\n    http_tokens                 = \"required\"\n    http_put_response_hop_limit = 2\n  }\n\n  user_data = <<-EOT\n    #!/bin/bash\n    set -euo pipefail\n    dnf -y update\n    systemctl enable --now amazon-ssm-agent || true\n    # Placeholder: install GitHub Actions runner / Ansible deps as needed.\n  EOT\n\n  tags = {\n    Name = \"${var.name}-ops\"\n    Role = \"ops\"\n  }\n}\n\n# --- VPC Endpoints (optional)\nresource \"aws_security_group\" \"endpoints\" {\n  count       = var.enable_vpc_endpoints ? 1 : 0\n  name        = \"${var.name}-vpce\"\n  description = \"Security group for interface VPC endpoints\"\n  vpc_id      = module.vpc.vpc_id\n\n  ingress {\n    description = \"HTTPS from VPC\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [module.vpc.vpc_cidr_block]\n  }\n\n  egress {\n    description = \"All egress\"\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_vpc_endpoint\" \"s3\" {\n  count             = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id            = module.vpc.vpc_id\n  service_name      = \"com.amazonaws.${var.aws_region}.s3\"\n  vpc_endpoint_type = \"Gateway\"\n  route_table_ids   = concat(module.vpc.private_route_table_ids, module.vpc.public_route_table_ids)\n}\n\nresource \"aws_vpc_endpoint\" \"ecr_api\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.ecr.api\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\nresource \"aws_vpc_endpoint\" \"ecr_dkr\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.ecr.dkr\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\nresource \"aws_vpc_endpoint\" \"logs\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.logs\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\nresource \"aws_vpc_endpoint\" \"ssm\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.ssm\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\nresource \"aws_vpc_endpoint\" \"ssmmessages\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.ssmmessages\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\nresource \"aws_vpc_endpoint\" \"ec2messages\" {\n  count               = var.enable_vpc_endpoints ? 1 : 0\n  vpc_id              = module.vpc.vpc_id\n  service_name        = \"com.amazonaws.${var.aws_region}.ec2messages\"\n  vpc_endpoint_type   = \"Interface\"\n  subnet_ids          = module.vpc.private_subnets\n  private_dns_enabled = true\n  security_group_ids  = [aws_security_group.endpoints[0].id]\n}\n\n# --- Optional Terraform state backend resources\nresource \"aws_s3_bucket\" \"tf_state\" {\n  count  = var.create_state_backend ? 1 : 0\n  bucket = var.state_bucket_name\n\n  force_destroy = false\n}\n\nresource \"aws_s3_bucket_versioning\" \"tf_state\" {\n  count  = var.create_state_backend ? 1 : 0\n  bucket = aws_s3_bucket.tf_state[0].id\n\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"tf_state\" {\n  count  = var.create_state_backend ? 1 : 0\n  bucket = aws_s3_bucket.tf_state[0].id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm     = \"aws:kms\"\n      kms_master_key_id = aws_kms_key.main.arn\n    }\n  }\n}\n\nresource \"aws_s3_bucket_public_access_block\" \"tf_state\" {\n  count  = var.create_state_backend ? 1 : 0\n  bucket = aws_s3_bucket.tf_state[0].id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n\nresource \"aws_dynamodb_table\" \"tf_lock\" {\n  count        = var.create_state_backend ? 1 : 0\n  name         = \"${var.name}-tf-lock\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n"
    },
    {
      "name": "outputs.tf",
      "content": "output \"vpc_id\" {\n  value       = module.vpc.vpc_id\n  description = \"VPC ID\"\n}\n\noutput \"private_subnet_ids\" {\n  value       = module.vpc.private_subnets\n  description = \"Private subnet IDs\"\n}\n\noutput \"control_plane_private_ips\" {\n  value       = [for i in aws_instance.control_plane : i.private_ip]\n  description = \"Private IPs of control plane instances\"\n}\n\noutput \"worker_asg_name\" {\n  value       = aws_autoscaling_group.workers.name\n  description = \"Worker Auto Scaling Group name\"\n}\n\noutput \"ops_instance_id\" {\n  value       = try(aws_instance.ops[0].id, null)\n  description = \"Ops host instance ID (if enabled)\"\n}\n"
    },
    {
      "name": "terraform.tfvars",
      "content": "aws_region = \"us-east-1\"\nname       = \"rke\"\n\n# Recommended: use SSM instead of SSH. If you must use SSH, set ssh_key_name and allowed_ssh_cidrs.\n# ssh_key_name      = \"my-keypair\"\n# allowed_ssh_cidrs  = [\"203.0.113.10/32\"]\n\naz_count            = 2\nvpc_cidr            = \"10.20.0.0/16\"\nsingle_nat_gateway  = true\nenable_vpc_endpoints = true\n\ncontrol_plane_count         = 1\ncontrol_plane_instance_type = \"t3.medium\"\n\nworker_min_size     = 1\nworker_desired_size = 1\nworker_max_size     = 3\nworker_instance_type = \"t3.large\"\n\nenable_ops_host   = true\nops_instance_type = \"t3.small\"\n\n# Optional: create remote state backend resources (bucket name must be globally unique)\ncreate_state_backend = false\n# state_bucket_name     = \"my-unique-tf-state-bucket-name\"\n"
    }
  ]
}